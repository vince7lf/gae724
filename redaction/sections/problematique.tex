\par Dans le cadre du projet pour PJCCI, une plateforme technologique sera mise à la disposition des gestionnaires du pont afin de les aider à prendre les décisions les plus responsables et raisonnables possibles. Mais la mise en opération d'une solution innovante et fiable, qui concilie des algorithmes d'apprentissage profond, du temps réel, des nano ordinateurs, et des conditions climatiques variables, est complexe. Dans une certaine mesure l'essai va contribuer à la recherche de solutions afin de répondre au défi pour le domaine du transport actif et durable d'être soutenu par des solutions technologiques fiables (opérationnelles), l'objectif étant de pouvoir offrir des services de qualité et sécuritaires sur l'ensemble des quatre saisons.
\par La seconde problématique que l'essai va contribuer à résoudre concerne les limites d'un nano ordinateur. Un nano ordinateur est un ordinateur miniaturisé en taille, mais aussi limité en capacité. Il existe différents fabricants et modèles, de spécifications variées, pour répondre à différents besoins. Le dernier né est le modèle "Jetson nano" du fabricant "NVIDIA ", disponible depuis juin 2019 au prix très abordable de 99\$US, et qui sera le matériel utilisé dans le cadre de cet essai. La compagnie NVIDIA a conçu ce matériel spécialement pour l'inférence de modèles d'apprentissage profond sur une plateforme mobile (drone) ou proche des données ("edge" en anglais). L'inférence nécessite une architecture et une puissance machine différente de celle nécessaire pour l'entrainement. Les modèles de réseaux de neurones sont adaptés et optimisés pour l'inférence. L'essai va permettre de préciser les capacités du Jetson nano pour l'inférence de diverses architectures de réseaux de neurones convolutifs entiers (FCN en anglais) et la segmentation sémantique en temps réel avec des vidéos de différentes propriétés (résolutions et nombre d'images pas seconde). Il existe des tests encourageants (\cite{nvidia_jetson_2019-1} \cite{nguyen_mavnet_2019} \cite{chong_real-time_1992}), qui seront utilisés comme modèle, même si ceux-ci sont limités à des types d'application qui ne sont pas les mêmes que pour l'essai.
\par Il est difficile de trouver des jeux de données pour entrainer les réseaux de neurones convolutifs entiers adaptés à la problématique. La technique de "Data augmentation" permet de démarrer d'un modèle qui a déjà appris avec un jeu d'images important (milliers d'images), et de lui faire apprendre davantage, en lui fournissant un plus petit jeu d'images (centaines d'images) de la nouvelle zone d'étude. Par exemple un modèle peut avoir appris à classifier des images de la Californie, États-Unis. Pour lui permettre de classifier des images de la Ville de Sherbrooke, il est souhaitable de lui fournir un nouveau jeu de données spécifique à cette ville afin qu'il s'adapte (ses paramètres) à cette région. Dans le contexte de cet essai, les données acquises sur le terrain seront fournies aux différents modèles qui seront évalués, et qui seront ré entrainés avec ce nouveau jeu d'images adapté à la zone d'étude.
\par La paramétrisation (des "hyper paramètres") des réseaux de neurones est très "subtile" et "intuitive" et requière de l'expérience. C'est un processus d'essais-erreurs qui est très couteux en temps, et risqué puisqu'il n'y a aucune garantie de succès. La technique de "Transfer Learning" permet d'hériter d'un modèle qui est déjà entrainé et configuré, et de l'adapter pour répondre à ses besoins. Cette technique permet un gain en temps et en énergie (et en argent) important puisque le temps de conception (architecture et configuration) et le temps d'entrainement, de validation et de tests sont diminués de façon non négligeable.
\begin{comment}
Par exemple le modèle "VGG" prend 2-3 semaines d'entrainement \cite{simonyan_very_2015} avec 4 GPU Titan Black (NVIDIA), coutant 1,200\$US (Amazon.com) chacun (pour un total de 4,800\$US, et cela juste pour les GPUs, qui ne sont qu'un des éléments de l'infrastructure nécessaire). Étant donné que de multiples tentatives sont nécessaires (cycles essai-erreur), la stratégie est d'entrainer plusieurs modèles en parallèle afin d'accélérer le développement, ce qui implique un cout élevé en infrastructure.
\end{comment}
La problématique pour l'essai est de trouver le modèle qui est le plus adapté pour répondre au besoin, et il en existe des milliers \cite{koh_model_2018}. La recherche dans la littérature permet heureusement de limiter les choix et donner des pistes (\cite{jia_real-time_2020} \cite{nguyen_mavnet_2019} \cite{nvidia_jetson_2019-1}). La problématique de la conception existe toujours, car le modèle a besoin d'être étudié, adapté et ré entrainé, jusqu'à l'obtention de résultats probants. Mais la paramétrisation des hyper paramètres n'est plus nécessaire (supposément), ce qui est très avantageux.
% \par Même si les tests du modèle donnent des résultats très satisfaisants en théorie, la réalité du terrain peut surprendre. Le test du modèle doit se faire dans des conditions réelles avec de nouvelles données (images), celles qui sont captées par le système hôte sur le terrain d'implantation: dans le jargon de l'intelligence artificielle, c'est l'"inférence" \cite{copel_whats_2016} \cite{nvidia_jetson_2019-1}. Il est assez probable que le modèle doive retourner à une phase d'adaptation. De plus, le système hôte, dans notre cas le nano-ordinateur NVIDIA Jetson nano, est conçu avec une architecture matérielle limitée (GPU, CPU, mémoire, taux de transfert, alimentation) et verra, au besoin, son architecture matérielle adaptée et remise en question.