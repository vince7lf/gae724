
@article{shahian_jahromi_real-time_2019,
  title = {Real-{{Time Hybrid Multi}}-{{Sensor Fusion Framework}} for {{Perception}} in {{Autonomous Vehicles}}},
  abstract = {There are many sensor fusion frameworks proposed in the literature using different sensors and fusion methods combinations and configurations. More focus has been on improving the accuracy performance; however, the implementation feasibility of these frameworks in an autonomous vehicle is less explored. Some fusion architectures can perform very well in lab conditions using powerful computational resources; however, in real-world applications, they cannot be implemented in an embedded edge computer due to their high cost and computational need. We propose a new hybrid multi-sensor fusion pipeline configuration that performs environment perception for autonomous vehicles such as road segmentation, obstacle detection, and tracking. This fusion framework uses a proposed encoder-decoder based Fully Convolutional Neural Network (FCNx) and a traditional Extended Kalman Filter (EKF) nonlinear state estimator method. It also uses a configuration of camera, LiDAR, and radar sensors that are best suited for each fusion method. The goal of this hybrid framework is to provide a cost-effective, lightweight, modular, and robust (in case of a sensor failure) fusion system solution. It uses FCNx algorithm that improve road detection accuracy compared to benchmark models while maintaining real-time efficiency that can be used in an autonomous vehicle embedded computer. Tested on over 3K road scenes, our fusion algorithm shows better performance in various environment scenarios compared to baseline benchmark networks. Moreover, the algorithm is implemented in a vehicle and tested using actual sensor data collected from a vehicle, performing real-time environment perception.},
  journal = {Sensors (Basel, Switzerland)},
  doi = {10.3390/s19204357},
  author = {Shahian Jahromi, B. and Tulabandhula, T. and Cetin, S.},
  year = {2019},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\NPGWDQBN\\Shahian Jahromi et al. - 2019 - Real-Time Hybrid Multi-Sensor Fusion Framework for.pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\U56VTENS\\display.html}
}

@article{fu_risk-based_2017,
  title = {A Risk-Based Approach to Winter Road Surface Condition Classification},
  abstract = {This paper presents a risk-based approach for classifying the road surface conditions of a highway network under winter weather events. A relative risk index (RRI) is developed to capture the effec..., Cet article pr{\'e}sente une approche fond{\'e}e sur le risque afin de classifier les conditions de la surface de roulement d'un r{\'e}seau d'autoroutes en fonction d'{\'e}v{\'e}nements m{\'e}t{\'e}orologiques d'hiver. On a d...},
  journal = {Canadian Journal of Civil Engineering},
  url = {https://www.nrcresearchpress.com/doi/full/10.1139/cjce-2016-0215},
  author = {Fu, Liping and Thakali, Lalita and Kwon, Tae J. and Usman, Taimur},
  month = mar,
  year = {2017},
  pages = {182-191},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\AYJZCB2M\\Fu et al. - 2017 - A risk-based approach to winter road surface condi.pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\VUVCL9AS\\cjce-2016-0215.html}
}

@article{cheng_road_2019,
  title = {Road Surface Condition Classification Using Deep Learning},
  abstract = {Traditional image recognition technology currently cannot achieve the fast real-time high-accuracy performance necessary for road recognition in intelligent driving. Deep learning models have been recently emerging as promising tools to achieve this performance. The recognition performance of such models can be boosted using appropriate selection of the activation functions. This paper proposes a deep learning approach for the classification of road surface conditions, and constructs a new activation function based on the rectified linear unit Rectified Linear Units (ReLu) activation function. The experimental results show a classification accuracy of 94.89\% on the road state database. Experiments on public datasets demonstrate that the proposed convolutional neural network model with the improved activation function has better generalization and excellent classification performance. \textcopyright{} 2019 Elsevier Inc.},
  journal = {Journal of Visual Communication and Image Representation},
  doi = {10.1016/j.jvcir.2019.102638},
  author = {Cheng, L. and Zhang, X. and Shen, J.},
  year = {2019},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\FAU2ET6P\\Cheng et al. - 2019 - Road surface condition classification using deep l.pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\9K9437RF\\display.html}
}

@article{pan_winter_nodate,
  title = {Winter {{Road Surface Condition Recognition Using}} a {{Pre}}-Trained {{Deep Convolutional Neural Network}}},
  author = {Pan, Guangyuan and Fu, Liping and Yu, Ruifan and Muresan, Matthew},
  pages = {13},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\BUXQ9J8E\\Pan et al. - Winter Road Surface Condition Recognition Using a .pdf}
}

@book{arai_advances_2019,
  title = {Advances in {{Computer Vision}}: {{Proceedings}} of the 2019 {{Computer Vision Conference}} ({{CVC}}), {{Volume}} 1},
  isbn = {978-3-030-17795-9},
  abstract = {This book presents a remarkable collection of chapters covering a wide range of topics in the areas of Computer Vision, both from theoretical and application perspectives. It gathers the proceedings of the Computer Vision Conference (CVC 2019), held in Las Vegas, USA from May 2 to 3, 2019. The conference attracted a total of 371 submissions from pioneering researchers, scientists, industrial engineers, and students all around the world. These submissions underwent a double-blind peer review process, after which 120 (including 7 poster papers) were selected for inclusion in these proceedings. The book's goal is to reflect the intellectual breadth and depth of current research on computer vision, from classical to intelligent scope. Accordingly, its respective chapters address state-of-the-art intelligent methods and techniques for solving real-world problems, while also outlining future research directions. Topic areas covered include Machine Vision and Learning, Data Science, Image Processing, Deep Learning, and Computer Vision Applications.},
  publisher = {{Springer}},
  url = {http://ebookcentral.proquest.com/lib/usherbrookemgh-ebooks/detail.action?docID=5755789},
  author = {Arai, Kohei and Kapoor, Supriya},
  year = {2019},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\BI2WIVP6\\detail.html;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\PMDL7DDW\\detail.html}
}

@article{lee_deep_2018,
  title = {{A Deep Neural Network Architecture for Real-Time Semantic Segmentation on Embedded Board}},
  abstract = {A Deep Neural Network Architecture for Real-Time Semantic Segmentation on Embedded Board deep learning;neural network;semantic segmentation;autonomous driving;embedded board; We propose Wide Inception ResNet (WIR Net) an optimized neural network architecture as a real-time semantic segmentation method for autonomous driving. The neural network architecture consists of an encoder that extracts features by applying a residual connection and inception module, and a decoder that increases the resolution by using transposed convolution and a low layer feature map. We also improved the performance by applying an ELU activation function and optimized the neural network by reducing the number of layers and increasing the number of filters. The performance evaluations used an NVIDIA Geforce GTX 1080 and TX1 boards to assess the class and category IoU for cityscapes data in the driving environment. The experimental results show that the accuracy of class IoU 53.4, category IoU 81.8 and the execution speed of \$640\{\textbackslash{}times\}360\$, \$720\{\textbackslash{}times\}480\$ resolution image processing 17.8fps and 13.0fps on TX1 board.},
  journal = {Journal of KIISE},
  url = {http://www.koreascience.or.kr/article/JAKO201823952431936.page},
  author = {Lee, J. and Lee, Y.},
  year = {2018},
  pages = {94-98}
}

@inproceedings{pathak_architecturally_2019,
  title = {Architecturally {{Compressed CNN}}: {{An Embedded Realtime Classifier}} ({{NXP Bluebox2}}.0 with {{RTMaps}})},
  abstract = {The convolution neural networks have revolutionized the computer vision domain. It has proven to be a dominant technology to carry out tasks such as image classification, semantic segmentation, and object detection. The convolution neural networks surpass the performance of the existing algorithms such as SIFT, HOG, etcetera. Where, instead of manually engineering the features, supervised learning help to learn the essential low-level and high-level features necessary for classifications. The convolution neural networks have become a popular tool to counter computer vision problems. However, it is computationally, and memory intensive to train and deploy the network because of the model size of a deep convolution neural networks. However, the research in the field of design space exploration (DSE) of neural networks and compression techniques to develop compact architectures, have made convolution neural networks memory and computationally efficient. These techniques have also improved the feasibility of convolution neural network for deployment on embedded targets. The paper explores the concept of compact convolution filters to reduce the number of parameters in a convolution neural network. The intuition behind the approach is that replacing convolution filters with a stack of compact convolution filters helps in developing a compact architecture with competitive accuracy. This paper explores the fire module a compact convolution filter and proposes a method of recreating a state-of-the-art architecture VGG-16 using the fire modules to develop a compact architecture, which is further trained on the CIFAR-10 dataset and deployed on a real-time embedded platform known as Bluebox 2.0 by NXP using RTMaps software framework.},
  booktitle = {2019 {{IEEE}} 9th {{Annual Computing}} and {{Communication Workshop}} and {{Conference}} ({{CCWC}})},
  doi = {10.1109/CCWC.2019.8666495},
  author = {Pathak, D. and {El-Sharkawy}, M.},
  month = jan,
  year = {2019},
  pages = {0331-0336},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\SHCWP9BF\\8666495.html}
}

@article{abouzahir_iot-empowered_2017,
  title = {{{IoT}}-Empowered Smart Agriculture: {{A}} Real-Time Light-Weight Embedded Segmentation System},
  abstract = {Internet of Things (IoT) is an emerging technology where standalone equipments and autonomous devices are connected to each other and users via Internet. When IoT concept meets agriculture, the future of farming is pushed to the next level, giving birth to what is called ``Smart Agriculture'' or ``Precision Agriculture''. The most important benefit from IoT is that a user can daily monitor his crop online in a seamless fashion. High quality data gathered from various sensors and transferred wirelessly to farm database will increase farmers understanding to their landuse leading to increasing income and product quality. One of the monitoring process is weeds detection and crop yield estimation using camera sensors. The acquired images help farmers to build map of weeds distribution or yield quantity all over the field, these maps can be used either for real-time processing or to predetermine weeds regions based on field maps history of the previous seasons. This process is referred to as segmentation problem. Several algorithms have been proposed for that purpose, however, these algorithms were run only on high performance computers. In this paper, we evaluate performance and the robustness of the most used legacy algorithms under local conditions. We focused on implementing these schemes within real-time application constraint. For instance, these algorithms were implemented and run in a low-cost embedded system. \textcopyright{} Springer International Publishing AG 2017.},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  doi = {10.1007/978-3-319-68179-5_28},
  author = {Abouzahir, S. and Sadik, M. and Sabir, E.},
  year = {2017},
  pages = {319-332},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\Q9CDUB9Y\\display.html}
}

@article{gilman_real-time_2014,
  title = {Real-Time Segmentation for Baggage Tracking on a Cost Effective Embedded Platform},
  abstract = {This paper describes segmentation and tracking parts of a machine vision based airport baggage tracking system. A simplified codebook based background subtraction method is used to segment the bag from a semi-static background. Morphological processing using an integral image is used to filter the foreground mask and the bag location is found using statistical methods. The system was implemented on a cost effective embedded processor and runs in real time at 30 fps. Five ARM based embedded platforms are evaluated and it is shown that all of them are capable of the required performance. Copyright \textcopyright{} 2014 Inderscience Enterprises Ltd.},
  journal = {International Journal of Intelligent Systems Technologies and Applications},
  doi = {10.1504/IJISTA.2014.068821},
  author = {Gilman, A. and Johnson, M.},
  year = {2014},
  pages = {245-257},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\Z3V5MJG2\\display.html}
}

@article{zhang_real-time_2012,
  title = {Real-Time Embedded Implementation of Foreground/Background Segmentation Algorithm},
  abstract = {Foreground/Background (FG/BG) segmentation is one of the most widely-used computer vision (CV) algorithms. Its basic algorithm takes advantages of correlation between background of different pixels or frames, predicts gray intensity for every pixel point and judges whether it belongs to foreground or background based on the prediction and actual value. Several FG/BG algorithms used in different situation were introduced firstly. Considering that those computationally expensive algorithms are hard to operate in real time using the traditional processor-based platform, its real-time embedded implementation was highlighted after this algorithm being verified. The design methodology and flow of reconfigurable computing (RC) which was an advanced method to implement those algorithms were introduced. In addition, several important issues about hardware implementation were discussed. A case of study to elaborate how it works was presented, and how to attach the implemented algorithm into the SoPC-based imaging system to achieve an integrated system was described.},
  journal = {Hongwai yu Jiguang Gongcheng/Infrared and Laser Engineering},
  author = {Zhang, H. and Chang, Y. and Li, F. and Shen, Y.},
  year = {2012},
  pages = {523-530},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\KU5XBSD7\\display.html}
}

@inproceedings{chougule_efficient_2018,
  title = {An Efficient Encoder-Decoder {{CNN}} Architecture for Reliable Multilane Detection in Real Time},
  abstract = {Multilane detection system is a vital prerequisite for realizing higher ADAS functionality of autonomous navigation. In this work, we present an efficient convolutional neural network (CNN) architecture for real time detection of multiple lane boundaries using a camera sensor. Our network has a simple encoder-decoder architecture and is a special two class semantic segmentation network designed to segment lane boundaries. Efficacy of our network stems from two key insights which are at the foundation of all our design decisions. Firstly, we term a lane boundary as a weak class object in the context of semantic segmentation. We show that the weak class objects which occupy relatively few pixels in the scene, also have a relatively low detection accuracy among the know segmentation methods. We present novel design choices and intuitions to improve the segmentation accuracy of weak class objects, which in turn reduces computation time. Our second insight lies in the manner we depict the ground truth information in our derived dataset. Instead of annotating just the visible lane markers, we accurately delineate the lane boundaries in the ground truth for challenging scenarios like occlusions, low light and degraded lane markings. We then leverage the CNN's ability to concisely summarize the global and local context in an image, for accurately inferring lane boundaries in these challenging cases. We evaluate our network against ENet and FCN-8, and found it performing notably better in terms of speed and accuracy. Our network achieves an encouraging 46 FPS performance on NVIDIA Drive PX2 platform and it has been validated on our test vehicle in highway driving conditions. \textcopyright{} 2018 IEEE.},
  booktitle = {{{IEEE Intelligent Vehicles Symposium}}, {{Proceedings}}},
  doi = {10.1109/IVS.2018.8500598},
  author = {Chougule, S. and Ismail, A. and Soni, A. and Kozonek, N. and Narayan, V. and Schulze, M.},
  year = {2018},
  pages = {1444-1451},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\E9NAPLX7\\display.html}
}

@misc{copel_whats_2016,
  title = {What's the {{Difference Between Deep Learning Training}} and {{Inference}}?},
  abstract = {Let's break let's break down the progression from deep-learning training to inference in the context of AI how they both function.},
  journal = {The Official NVIDIA Blog},
  url = {https://blogs.nvidia.com/blog/2016/08/22/difference-deep-learning-training-inference-ai/},
  author = {Copel, M.},
  month = aug,
  year = {2016},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\2HR5PN5M\\difference-deep-learning-training-inference-ai.html;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\R62EH9TZ\\difference-deep-learning-training-inference-ai.html}
}

@article{chong_real-time_1992,
  title = {Real-Time Edge Detection and Image Segmentation},
  abstract = {The implementation of real-time edge detection and image segmentation using analog VLSI is described. A novel technique of image segmentation using radially propagating signals is discussed. Current-mode processing is used to avoid offset voltages and lead to circuit implementations which are compatible with standard CMOS processes. A system using raylike signal propagation and implemented using 3{$\mu$}-CMOS technology is described together with experimental results. \textcopyright{} 1992 Kluwer Academic Publishers.},
  journal = {Analog Integrated Circuits and Signal Processing},
  doi = {10.1007/BF00142412},
  author = {Chong, C. P. and Salama, C. A. T. and Smith, K. C.},
  year = {1992},
  pages = {117-130},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\KN2ABNHF\\display.html}
}

@article{bernas_edge_2017,
  title = {Edge {{Real}}-{{Time Medical Data Segmentation}} for {{IoT Devices}} with {{Computational}} and {{Memory Constrains}}},
  abstract = {The Internet of Things (IoT) becomes very important tool for data gathering and management in many environments. The majority of dedicated solutions register data only at time of events, while in case of medical data full records for long time periods are usually needed. The precision of acquired data and the amount of data sent by sensor-equipped IoT devices has vital impact on lifetime of these devices. In case of solutions, where multiple sensors are available for single device with limited computation power and memory, the complex compression or transformation methods cannot be applied - especially in case of nano device injected to a body. Thus this paper is focused on linear complexity segmentation algorithms that can be used by the resource-limited devices. The state-of-art data segmentation methods are analysed and adapted for simple IoT devices. Two segmentation algorithms are proposed and tested on a real-world dataset collected from a prototype of the IoT device. \textcopyright{} 2017, Springer International Publishing AG.},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  doi = {10.1007/978-3-319-67077-5_12},
  author = {Bernas, M. and P{\l}aczek, B. and Sapek, A.},
  year = {2017},
  pages = {119-128},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\FNZJGH7U\\display.html}
}

@article{blanco-filgueira_deep_2019,
  title = {Deep {{Learning}}-{{Based Multiple Object Visual Tracking}} on {{Embedded System}} for {{IoT}} and {{Mobile Edge Computing Applications}}},
  abstract = {Compute and memory demands of state-of-the-art deep learning methods are still a shortcoming that must be addressed to make them useful at Internet of Things (IoT) end-nodes. In particular, recent results depict a hopeful prospect for image processing using convolutional neural networks, CNNs, but the gap between software and hardware implementations is already considerable for IoT and mobile edge computing applications due to their high power consumption. This proposal performs low-power and real time deep learning-based multiple object visual tracking implemented on an NVIDIA Jetson TX2 development kit. It includes a camera and wireless connection capability and it is battery powered for mobile and outdoor applications. A collection of representative sequences captured with the on-board camera, dETRUSC video dataset, is used to exemplify the performance of the proposed algorithm and to facilitate benchmarking. The results in terms of power consumption and frame rate demonstrate the feasibility of deep learning algorithms on embedded platforms although more effort in the joint algorithm and hardware design of CNNs is needed.},
  journal = {IEEE Internet of Things Journal},
  doi = {10.1109/JIOT.2019.2902141},
  author = {{Blanco-Filgueira}, B. and {Garc{\'i}a-Lesta}, D. and {Fern{\'a}ndez-Sanjurjo}, M. and Brea, V. M. and L{\'o}pez, M.},
  month = jun,
  year = {2019},
  pages = {5423-5431},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\Z6MFFWVE\\Blanco-Filgueira et al. - 2019 - Deep Learning-Based Multiple Object Visual Trackin.pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\DXT2Z9YY\\8653851.html}
}

@article{nguyen_mavnet_2019,
  archivePrefix = {arXiv},
  primaryClass = {cs},
  title = {{{MAVNet}}: An {{Effective Semantic Segmentation Micro}}-{{Network}} for {{MAV}}-Based {{Tasks}}},
  abstract = {Real-time semantic image segmentation on platforms subject to size, weight and power (SWaP) constraints is a key area of interest for air surveillance and inspection. In this work, we propose MAVNet: a small, light-weight, deep neural network for real-time semantic segmentation on micro Aerial Vehicles (MAVs). MAVNet, inspired by ERFNet [1], features 400 times fewer parameters and achieves comparable performance with some reference models in empirical experiments. Our model achieves a trade-off between speed and accuracy, achieving up to 48 FPS on an NVIDIA 1080Ti and 9 FPS on the NVIDIA Jetson Xavier when processing high resolution imagery. Additionally, we provide two novel datasets that represent challenges in semantic segmentation for real-time MAV tracking and infrastructure inspection tasks and verify MAVNet on these datasets. Our algorithm and datasets are made publicly available.},
  journal = {arXiv:1904.01795 [cs]},
  url = {http://arxiv.org/abs/1904.01795},
  author = {Nguyen, T. and Shivakumar, S. S. and Miller, I. D. and Keller, J. and Lee, E. S. and Zhou, Alex and Ozaslan, Tolga and Loianno, Giuseppe and Harwood, Joseph H. and Wozencraft, Jennifer and Taylor, Camillo J. and Kumar, Vijay},
  month = jun,
  year = {2019},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\LTDNIYFE\\Nguyen et al. - 2019 - MAVNet an Effective Semantic Segmentation Micro-N.pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\DFUUHCWL\\1904.html}
}

@article{orsic_defense_2019,
  archivePrefix = {arXiv},
  primaryClass = {cs},
  title = {In {{Defense}} of {{Pre}}-Trained {{ImageNet Architectures}} for {{Real}}-Time {{Semantic Segmentation}} of {{Road}}-Driving {{Images}}},
  abstract = {Recent success of semantic segmentation approaches on demanding road driving datasets has spurred interest in many related application fields. Many of these applications involve real-time prediction on mobile platforms such as cars, drones and various kinds of robots. Real-time setup is challenging due to extraordinary computational complexity involved. Many previous works address the challenge with custom lightweight architectures which decrease computational complexity by reducing depth, width and layer capacity with respect to general purpose architectures. We propose an alternative approach which achieves a significantly better performance across a wide range of computing budgets. First, we rely on a light-weight general purpose architecture as the main recognition engine. Then, we leverage light-weight upsampling with lateral connections as the most cost-effective solution to restore the prediction resolution. Finally, we propose to enlarge the receptive field by fusing shared features at multiple resolutions in a novel fashion. Experiments on several road driving datasets show a substantial advantage of the proposed approach, either with ImageNet pre-trained parameters or when we learn from scratch. Our Cityscapes test submission entitled SwiftNetRN-18 delivers 75.5\% MIoU and achieves 39.9 Hz on 1024\texttimes{}2048 images on GTX1080Ti.},
  journal = {arXiv:1903.08469 [cs]},
  url = {http://arxiv.org/abs/1903.08469},
  author = {Or{\v s}i{\'c}, M. and Kre{\v s}o, I. and Bevandi{\'c}, P. and {\v S}egvi{\'c}, S.},
  month = apr,
  year = {2019},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\3NAX2RKH\\Oršić et al. - 2019 - In Defense of Pre-trained ImageNet Architectures f.pdf}
}

@article{spek_cream_2018,
  archivePrefix = {arXiv},
  primaryClass = {cs},
  title = {{{CReaM}}: {{Condensed Real}}-Time {{Models}} for {{Depth Prediction}} Using {{Convolutional Neural Networks}}},
  abstract = {Since the resurgence of CNNs the robotic vision community has developed a range of algorithms that perform classification, semantic segmentation and structure prediction (depths, normals, surface curvature) using neural networks. While some of these models achieve state-of-the art results and super human level performance, deploying these models in a time critical robotic environment remains an ongoing challenge. Real-time frameworks are of paramount importance to build a robotic society where humans and robots integrate seamlessly. To this end, we present a novel real-time structure prediction framework that predicts depth at 30 frames per second on an NVIDIA-TX2. At the time of writing, this is the first piece of work to showcase such a capability on a mobile platform. We also demonstrate with extensive experiments that neural networks with very large model capacities can be leveraged in order to train accurate condensed model architectures in a ``from teacher to student'' style knowledge transfer.},
  journal = {arXiv:1807.08931 [cs]},
  url = {http://arxiv.org/abs/1807.08931},
  author = {Spek, A. and Dharmasiri, T. and Drummond, T.},
  month = jul,
  year = {2018},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\TLMP9DKN\\Spek et al. - 2018 - CReaM Condensed Real-time Models for Depth Predic.pdf}
}

@misc{dettmers_deep_2015,
  title = {Deep {{Learning}} in a {{Nutshell}}: {{History}} and {{Training}}},
  abstract = {Part 2 of an intuitive and gentle introduction to deep learning. Covers the most important deep learning concepts, giving an understanding rather than mathematical and theoretical details.},
  journal = {NVIDIA Developer Blog},
  url = {https://devblogs.nvidia.com/deep-learning-nutshell-history-training/},
  author = {Dettmers, T.},
  month = dec,
  year = {2015},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\5JDY9ZLF\\deep-learning-nutshell-history-training.html}
}

@misc{jiaconda_concise_2019,
  title = {A {{Concise History}} of {{Neural Networks}}},
  abstract = {``From the barren landscapes inside our personal devices come furtive anthems hummed by those digital servants who will one day be our\ldots{}},
  journal = {Medium},
  url = {https://towardsdatascience.com/a-concise-history-of-neural-networks-2070655d3fec},
  author = {Jiaconda},
  month = apr,
  year = {2019},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\I5J2KY8U\\a-concise-history-of-neural-networks-2070655d3fec.html}
}

@misc{kurenkov_brief_2015,
  title = {A '{{Brief}}' {{History}} of {{Neural Nets}} and {{Deep Learning}}},
  abstract = {The beginning of a story spanning half a century, about how we learned to make computers learn},
  journal = {Andrey Kurenkov's Web World},
  url = {https://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning/},
  author = {Kurenkov, A.},
  year = {2015},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\P8293RRZ\\a-brief-history-of-neural-nets-and-deep-learning.html}
}

@misc{beam_deep_2017,
  title = {Deep {{Learning}} 101 - {{Part}} 1: {{History}} and {{Background}}},
  url = {https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html},
  author = {Beam, A.},
  year = {2017},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\B6KZDFXT\\deep_learning_101_part1.html;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\KM8S8N9Y\\deep-learning-101.html}
}

@incollection{gaweda_deep_2018,
  title = {Deep {{Neural Networks}}\textemdash{{A Brief History}}},
  isbn = {978-3-319-67945-7 978-3-319-67946-4},
  booktitle = {Advances in {{Data Analysis}} with {{Computational Intelligence Methods}}},
  publisher = {{Springer International Publishing}},
  url = {http://link.springer.com/10.1007/978-3-319-67946-4_7},
  author = {Cios, K. J.},
  editor = {Gaw{\k{e}}da, A. E. and Kacprzyk, J. and Rutkowski, L. and Yen, G. G.},
  year = {2018},
  pages = {183-200},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\F72FW5K7\\Cios - 2018 - Deep Neural Networks—A Brief History.pdf},
  doi = {10.1007/978-3-319-67946-4_7}
}

@article{nielsen_neural_2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  url = {http://neuralnetworksanddeeplearning.com},
  author = {Nielsen, M. A.},
  year = {2015},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\EW5MS3VD\\neuralnetworksanddeeplearning.com.html;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\L97LNVGT\\index.html}
}

@book{goodfellow_deep_2016,
  title = {Deep {{Learning}}},
  url = {http://www.deeplearningbook.org/},
  author = {Goodfellow, I. and Bengio, Y. and Courville, A.},
  year = {2016},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\3J6JEMZX\\Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT).pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\V89SUNQP\\www.deeplearningbook.org.html}
}

@misc{nvidia_nvidia_2015,
  title = {{{NVIDIA Deep Learning}}, {{AI}}, \& {{HPC Classes}} \& {{Workshops}}},
  abstract = {Find hands-on AI and Accelerated Computing courses and events.},
  journal = {NVIDIA},
  url = {https://www.nvidia.com/en-us/deep-learning-ai/education/},
  author = {NVIDIA},
  year = {2015},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\A4ZP642G\\education.html}
}

@misc{nvidia_nvidia_2015-1,
  title = {{{NVIDIA Teaching Kits}}},
  abstract = {NVIDIA Teaching Kits are complete course solutions for use by educators in a variety of academic disciplines that benefit from GPU-accelerated computing. Co-developed with leading university faculty, Teaching Kits provide full curriculum design coupled with ease-of-use. Educators can bridge academic theory with real-world application to empower next-generation innovators with critical AI skillsets.},
  journal = {NVIDIA Developer},
  url = {https://developer.nvidia.com/teaching-kits},
  author = {NVIDIA},
  month = nov,
  year = {2015},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\VU8KUCTL\\teaching-kits.html}
}

@book{bishop_pattern_2006,
  series = {Information Science and Statistics},
  title = {Pattern Recognition and Machine Learning},
  isbn = {978-0-387-31073-2},
  lccn = {Q327 .B52 2006},
  publisher = {{Springer}},
  author = {Bishop, C. M.},
  year = {2006},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\JXDXA9PY\\Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@misc{pytorch.com_welcome_2017,
  title = {Welcome to {{PyTorch Tutorials}} \textemdash{} {{PyTorch Tutorials}} 1.3.0 Documentation},
  url = {https://pytorch.org/tutorials/},
  author = {PyTorch.com},
  year = {2017},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\I9NR9WVG\\tutorials.html}
}

@book{sharma_history_2019,
  series = {Intelligent {{Systems Reference Library}}},
  title = {The History, Present and Future with Iot},
  isbn = {18684394 (ISSN)},
  abstract = {Human beings quest for making comfortable life is due to their inquisitiveness about technical arena. Over the last few decades, mankind had experienced technical transformational journey with the inventions of new technology frontiers. These frontiers have interacted with human beings and performed every possible work in shorter period of time and with a much greater accuracy. With the advent of `Smart Concepts', the world is now becoming more connected. Precisely termed as hyper-connected world. The smart concepts includes smart phones, smart devices, smart applications and smart cities. These smarter concepts forms an ecosystem of devices whose basic work is to connect various devices to send and receive data. Internet of Things is one the dominating technology that keeps eye on the connected smart devices.~Internet of Things has bought applications from fiction to fact enabling fourth industrial revolution.~It has laid an incredible impact on the technical, social, economic and on the lives of human and machines. Scientists claim that the potential benefit derived~from this technology will sprout a foreseeable future where the smart objects sense, think and act. Internet of Things is the trending technology and embodies various concepts such as fog computing, edge computing, communication protocols, electronic devices, sensors, geo-location etc. The chapter presents the comprehensive information about the evolution of Internet of Things, its present developments to its futuristic applications. \textcopyright{} Springer Nature Switzerland AG 2019.},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060126104\&doi=10.1007\%2f978-3-030-04203-5_3\&partnerID=40\&md5=1878f15afc39fbca5142a5f680e0f3c7},
  author = {Sharma, N. and Shamkuwar, M. and Singh, I.},
  year = {2019},
  keywords = {Sensors,Internet of things,IoT,Communication model,Edge computing,Fog computing,Future of IoT,IoT applications,IoT architecture,IoT definition,IoT evolution,IoT history,IoT technologies,IoT trends},
  doi = {10.1007/978-3-030-04203-5_3}
}

@misc{nvidia_jetson_2019,
  title = {Jetson {{Nano}}},
  abstract = {Jetson Nano is a small, powerful computer for embedded applications and AI IoT that delivers the power of modern AI in a \$129 module. Get started fast with the comprehensive JetPack SDK with accelerated libraries for deep learning, computer vision, graphics, multimedia, and more.},
  journal = {NVIDIA Developer},
  url = {https://developer.nvidia.com/embedded/jetson-nano},
  author = {NVIDIA},
  month = mar,
  year = {2019},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\M2RQGN97\\jetson-nano.html}
}

@incollection{jia_real-time_2020,
  title = {Real-{{Time Semantic Segmentation Network}} for {{Edge Deployment}}},
  isbn = {978-981-329-697-8 978-981-329-698-5},
  abstract = {In this paper, we focus on the embedded deployment of real-time semantic segmentation network. Semantic segmentation based on convolutional neural networks has achieved impressive success in recent years, thus raising interests of researchers in many related application fields. The practical applications such as autonomous driving raise challenge to the lightweight of networks. Many previous achievements lighten the network by reducing layers, channels and applying group convolution, depthwise convolution to get realtime performance. At a cost, the learning ability of the network decreases. To break out of the dilemma, we propose MFANet with efficient multi-fiber unit and attention module, which obtains well balance between speed and performance. Testing on single NVIDIA 1080Ti GPU, the network achieves 65.71\% MIoU with only 3.472 GFLOPs and speed of 135 FPS on the Cityscapes dataset as the input size 512 {\^A} 1024. Further on, after some adjustments, we deploy the network to NVIDIA jetson nano embedded system, it achieves 55.33\% MIoU and speed of 12 FPS, to further accelerate the model, we converted the trained model into tensorrt model of type int8, it achieves 54. 47\% MIoU and speed of 47 FPS, which is capable of industrial deployment.},
  booktitle = {Proceedings of 2019 {{Chinese Intelligent Systems Conference}}},
  publisher = {{Springer Singapore}},
  url = {http://link.springer.com/10.1007/978-981-32-9698-5_28},
  author = {Zheng, J. and Li, J. and Liu, Y. and Zhang, W.},
  editor = {Jia, Y. and Du, J. and Zhang, W.},
  year = {2020},
  pages = {243-249},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\EPPLCC6C\\Zheng et al. - 2020 - Real-Time Semantic Segmentation Network for Edge D.pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\SZSDSH7V\\335687380_Real-Time_Semantic_Segmentation_Network_for_Edge_Deployment.html},
  doi = {10.1007/978-981-32-9698-5_28}
}

@inproceedings{mody_low_2018,
  title = {Low Cost and Power {{CNN}}/Deep Learning Solution for Automated Driving},
  abstract = {Automated driving functions, like highway driving and parking assist, are increasingly getting deployed in high-end cars with the ultimate goal of realizing self-driving car using Deep learning techniques like convolution neural network (CNN). For mass-market deployment, the embedded solution is required to address the right cost and performance envelope along with security and safety. In the case of automated driving, one of the key functionality is 'finding drivable free space', which is addressed using deep learning techniques like CNN. These CNN networks pose huge computing requirements in terms of hundreds of GOPS/TOPS (Giga or Tera operations per second), which seems beyond the capability of today's embedded SoC. This paper covers various techniques consisting of fixed-point conversion, sparse multiplication, fusing of layers and network pruning, for tailoring on the embedded solution. These techniques are implemented on the device by means of optimized Deep learning library for inference. The paper concludes by demonstrating the results of a CNN network running in real time on TI's TDA2X embedded platform producing a high-quality drivable space output for automated driving. \textcopyright{} 2018 IEEE.},
  booktitle = {Proceedings - {{International Symposium}} on {{Quality Electronic Design}}, {{ISQED}}},
  doi = {10.1109/ISQED.2018.8357325},
  author = {Mody, M. and Kumar, D. and Swami, P. and Mathew, M. and Nagori, S.},
  year = {2018},
  pages = {432-436},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\TVV5H8AR\\display.html}
}

@article{simonyan_very_2015,
  archivePrefix = {arXiv},
  primaryClass = {cs},
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 \texttimes{} 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16\textendash{}19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  journal = {arXiv:1409.1556 [cs]},
  url = {http://arxiv.org/abs/1409.1556},
  author = {Simonyan, K. and Zisserman, A.},
  month = apr,
  year = {2015},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\ZG8HWFYC\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf}
}

@misc{koh_model_2018,
  title = {Model {{Zoo}} - {{Deep}} Learning Code and Pretrained Models for Transfer Learning, Educational Purposes, and More},
  url = {https://modelzoo.co/},
  author = {Koh, J. Y.},
  year = {2018},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\RFNXQS62\\modelzoo.co.html}
}

@misc{nvidia_jetson_2019-1,
  title = {Jetson {{Nano}}: {{Deep Learning Inference Benchmarks}}},
  abstract = {Jetson Nano can run a wide variety of advanced networks, including the full native versions of popular ML frameworks like TensorFlow, PyTorch, Caffe/Caffe2, Keras, MXNet, and others. These networks can be used to build autonomous machines and complex AI systems by implementing robust capabilities such as image recognition, object detection and localization, pose estimation, semantic segmentation, video enhancement, and intelligent analytics. To run the following benchmarks on your Jetson Nano, please see the instructions here.},
  journal = {NVIDIA Developer},
  url = {https://developer.nvidia.com/embedded/jetson-nano-dl-inference-benchmarks},
  author = {NVIDIA},
  month = apr,
  year = {2019},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\32AJUXPV\\jetson-nano-dl-inference-benchmarks.html}
}

@inproceedings{mountelos_vehicle_2019,
  title = {Vehicle {{Windshield Detection}} by {{Fast}} and {{Compact Encoder}}-{{Decoder FCN Architecture}}},
  abstract = {Vehicle semantic segmentation is critical in many advanced driving assistance systems, traffic management, and security surveillance systems. Most of such systems are deployed on low computational embedded systems located in the vehicles or in remote gantry and roadside poles. While fully convolutional networks have been proved to be a powerful classifier being able to make inference on every single pixel of the input image, they entail high computational costs even for the inference process. In this paper, a vehicle windshield semantic segmentation is proposed utilizing a fast and compact encoder-decoder architecture of a fully convolutional network implemented in a low-power embedded system. The performed qualitative and quantitative performance measurements exemplify a real-time portable embedded solution which is competitive in terms of performance and inference time.},
  booktitle = {2019 8th {{International Conference}} on {{Modern Circuits}} and {{Systems Technologies}} ({{MOCAST}})},
  doi = {10.1109/MOCAST.2019.8741770},
  author = {Mountelos, A. and Amanatiadis, A. and Sirakoulis, G. and Kosmatopoulos, E. B.},
  month = may,
  year = {2019},
  pages = {1-4},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\S3Y6BBM3\\Mountelos et al. - 2019 - Vehicle Windshield Detection by Fast and Compact E.pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\68CRTGWX\\8741770.html}
}

@inproceedings{bidyanta_real-time_2018,
  title = {Real-{{Time GPU Based Video Segmentation}} with {{Depth Information}}},
  abstract = {In the context of video segmentation with depth sensor, prior work maps the Metropolis algorithm, a simulated annealing based key routine during segmentation, onto an Nvidia Graphics Processing Unit (GPU) and achieves real-time performance for 320\texttimes{}256 video sequences. However that work utilizes depth information in a very limited manner. This paper presents a new GPU-based method that expands the use of depth information during segmentation and shows the improved segmentation quality over the prior work. In particular, we discuss various ways to restructure the segmentation flow, and evaluate the impact of several design choices on throughput and quality. We introduce a scaling factor for amplifying the interaction strength between two spatially neighboring pixels and increasing the clarity of borderlines. This allows us to reduce the number of required Metropolis iterations by over 50\% with the drawback of over-segmentation. We evaluate two design choices to overcome this problem. First, we incorporate depth information into the perceived color difference calculations between two pixels, and show that the interaction strengths between neighboring pixels can be more accurately modeled by incorporating depth information. Second, we pre-process the frames with Bilateral filter instead of Gaussian filter, and show its effectiveness in terms of reducing the difference between similar colors. Both approaches help improve the quality of the segmentation, and the reduction in Metropolis iterations helps improve the throughout from 29 fps to 34 fps for 320\texttimes{}256 video sequences.},
  booktitle = {2018 {{IEEE}}/{{ACS}} 15th {{International Conference}} on {{Computer Systems}} and {{Applications}} ({{AICCSA}})},
  doi = {10.1109/AICCSA.2018.8612854},
  author = {Bidyanta, N. and Akoglu, A.},
  month = oct,
  year = {2018},
  pages = {1-8},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\7K3I2WWV\\Bidyanta and Akoglu - 2018 - Real-Time GPU Based Video Segmentation with Depth .pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\N9VP9ZF5\\8612854.html}
}

@article{paszke_enet_2016,
  archivePrefix = {arXiv},
  primaryClass = {cs},
  title = {{{ENet}}: {{A Deep Neural Network Architecture}} for {{Real}}-{{Time Semantic Segmentation}}},
  abstract = {The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18\texttimes{} faster, requires 75\texttimes{} less FLOPs, has 79\texttimes{} less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.},
  journal = {arXiv:1606.02147 [cs]},
  url = {http://arxiv.org/abs/1606.02147},
  author = {Paszke, A. and Chaurasia, A. and Kim, S. and Culurciello, E.},
  month = jun,
  year = {2016},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\IZ2AMWJY\\Paszke et al. - 2016 - ENet A Deep Neural Network Architecture for Real-.pdf}
}

@inproceedings{runz_co-fusion_2017,
  title = {Co-Fusion: {{Real}}-Time Segmentation, Tracking and Fusion of Multiple Objects},
  isbn = {978-1-5090-4633-1},
  abstract = {In this paper we introduce Co-Fusion, a dense SLAM system that takes a live stream of RGB-D images as input and segments the scene into different objects (using either motion or semantic cues) while simultaneously tracking and reconstructing their 3D shape in real time. We use a multiple model fitting approach where each object can move independently from the background and still be effectively tracked and its shape fused over time using only the information from pixels associated with that object label. Previous attempts to deal with dynamic scenes have typically considered moving regions as outliers, and consequently do not model their shape or track their motion over time. In contrast, we enable the robot to maintain 3D models for each of the segmented objects and to improve them over time through fusion. As a result, our system can enable a robot to maintain a scene description at the object level which has the potential to allow interactions with its working environment; even in the case of dynamic scenes.},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  publisher = {{IEEE}},
  url = {http://ieeexplore.ieee.org/document/7989518/},
  author = {Runz, M. and Agapito, L.},
  month = may,
  year = {2017},
  pages = {4471-4478},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\9464BKRS\\Runz and Agapito - 2017 - Co-fusion Real-time segmentation, tracking and fu.pdf}
}

@inproceedings{long_fully_2015,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build ``fully convolutional'' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  doi = {10.1109/CVPR.2015.7298965},
  author = {Long, J. and Shelhamer, E. and Darrell, T.},
  month = jun,
  year = {2015},
  pages = {3431-3440},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\L4LUDNMZ\\Long et al. - 2015 - Fully convolutional networks for semantic segmenta.pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\ZSU6X84H\\7298965.html}
}

@article{redmon_you_2016,
  archivePrefix = {arXiv},
  primaryClass = {cs},
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  journal = {arXiv:1506.02640 [cs]},
  url = {http://arxiv.org/abs/1506.02640},
  author = {Redmon, J. and Divvala, S. and Girshick, R. and Farhadi, A.},
  month = may,
  year = {2016},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\WEW3VC73\\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\ES2RK2MI\\1506.html;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\MTSW8HB5\\yolo.html}
}

@book{ramsundar_tensorflow_2018,
  edition = {1st},
  title = {{{TensorFlow}} for {{Deep Learning}}: {{From Linear Regression}} to {{Reinforcement Learning}}},
  isbn = {978-1-4919-8045-3},
  abstract = {Learn how to solve challenging machine learning problems with TensorFlow, Googles revolutionary new software library for deep learning. If you have some background in basic linear algebra and calculus, this practical book introduces machine-learning fundamentals by showing you how to design systems capable of detecting objects in images, understanding text, analyzing video, and predicting the properties of potential medicines. TensorFlow for Deep Learning teaches concepts through practical examples and helps you build knowledge of deep learning foundations from the ground up. Its ideal for practicing developers with experience designing software systems, and useful for scientists and other professionals familiar with scripting but not necessarily with designing learning algorithms. Learn TensorFlow fundamentals, including how to perform basic computation Build simple learning systems to understand their mathematical foundations Dive into fully connected deep networks used in thousands of applications Turn prototypes into high-quality models with hyperparameter optimization Process images with convolutional neural networks Handle natural language datasets with recurrent neural networks Use reinforcement learning to solve games such as tic-tac-toe Train deep networks with hardware including GPUs and tensor processing units},
  publisher = {{O'Reilly Media, Inc.}},
  author = {Ramsundar, Bharath and Zadeh, Reza Bosagh},
  year = {2018},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\KFCBL46V\\Zadeh and Ramsundar - TensorFlow for Deep Learning.pdf}
}

@article{alom_history_2018-1,
  archivePrefix = {arXiv},
  primaryClass = {cs},
  title = {The {{History Began}} from {{AlexNet}}: {{A Comprehensive Survey}} on {{Deep Learning Approaches}}},
  abstract = {Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].},
  journal = {arXiv:1803.01164 [cs]},
  url = {http://arxiv.org/abs/1803.01164},
  author = {Alom, Md Zahangir and Taha, Tarek M. and Yakopcic, Christopher and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Van Esesn, Brian C. and Awwal, Abdul A. S. and Asari, Vijayan K.},
  month = sep,
  year = {2018},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\7QJF7Q6A\\Alom et al. - 2018 - The History Began from AlexNet A Comprehensive Su.pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\2ASK8LWZ\\1803.html}
}

@article{edge_exploring_2018,
  title = {Exploring E-Bikes as a Mode of Sustainable Transport: {{A}} Temporal Qualitative Study of the Perspectives of a Sample of Novice Riders in a {{Canadian}} City},
  copyright = {\textcopyright{} 2018 Canadian Association of Geographers / L'Association canadienne des g{\'e}ographes},
  abstract = {Key Messages E-bikes are an under-studied emerging technology with potential to facilitate sustainability agendas in urban transportation (e.g. active, multi-modal, low-carbon mobility). Little to no research has been conducted on e-bikes in Canadian urban contexts where sprawl and car dependency are predominant. E-bikes may be a viable substitute for cars for shorter commutes, yet weight, winter weather, and battery life remain barriers.},
  journal = {The Canadian Geographer / Le G{\'e}ographe canadien},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/cag.12456},
  author = {Edge, S. and Dean, J. and Cuomo, M. and Keshav, S.},
  year = {2018},
  pages = {384-397},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\PCTN2392\\Edge et al. - 2018 - Exploring e-bikes as a mode of sustainable transpo.pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\9RTK88GV\\cag.html}
}

@misc{government_of_canada_daily_2017,
  title = {The {{Daily}} \textemdash{} {{Journey}} to Work: {{Key}} Results from the 2016 {{Census}}},
  abstract = {More Canadians commuted to work in 2016 and a greater proportion took public transit than ever before. Since 1996, the number of commuters has risen by 3.7 million or 30.3\% to 15.9 million in 2016. However, how they get to work is changing.},
  url = {http://www150.statcan.gc.ca/n1/daily-quotidien/171129/dq171129c-eng.htm},
  author = {{Government of Canada}, Statistics Canada},
  month = nov,
  year = {2017},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\7IEKPWEP\\dq171129c-eng.html}
}

@article{cabral_bicycle_2018,
  title = {Bicycle Ridership and Intention in a Northern, Low-Cycling City},
  abstract = {Cycling as a mode of transportation (i.e. utility cycling) has been given heavy attention and investment in North America over the last decade. It is perceived as an environmentally friendly way to travel, leading to benefits for health and traffic alleviation. This study examines the determinants of utility cycling behaviour and intent, and more broadly, active transportation (i.e. cycling and walking) behaviour in Edmonton, Canada \textendash{} the northernmost North American city with a metropolitan population over one million. With harsh winter weather and low cycling rates, the city presents a unique case study for cycling behavior. In this research, we analyzed 646 responses to a bike ridership survey conducted in 2014 by the City of Edmonton. Borrowing concepts from behaviour theory, public health and transportation engineering we seek to quantify the effects of infrastructure density, traffic attitude, perceived control over time and distance, and traffic stress tolerance perception on cycling for utility purposes, the intention to cycle more frequently, and the use of an active mode of transportation, specifically for a northern and low-cycling city. Three empirical models were developed to describe cycling behaviour using binary logistic regression. Most variables were significant and in line with other study findings in the current literature. Results point at the importance of perceived safety in deciding or intending to cycle, as well as perceived time and distance of travel. Broad policy implications and suggestions for future research are discussed.},
  journal = {Travel Behaviour and Society},
  url = {http://www.sciencedirect.com/science/article/pii/S2214367X18300565},
  author = {Cabral, Laura and Kim, Amy M. and Parkins, John R.},
  month = oct,
  year = {2018},
  pages = {165-173},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\M5KN22TZ\\Cabral et al. - 2018 - Bicycle ridership and intention in a northern, low.pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\DJV66EMH\\S2214367X18300565.html}
}

@article{schneider_theory_2013,
  title = {Theory of Routine Mode Choice Decisions: {{An}} Operational Framework to Increase Sustainable Transportation},
  abstract = {A growing number of communities in the United States are seeking to improve the sustainability of their transportation systems by shifting routine automobile travel to walking and bicycling. In order to identify strategies that may be most effective at increasing pedestrian and bicycle transportation in a specific local context, practitioners need a greater understanding of the underlying thought process that people use to select travel modes. Previous research from the travel behavior and psychology fields provides the foundation for a five-step, operational Theory of Routine Mode Choice Decisions. Walking and bicycling could be promoted through each of the five steps: awareness and availability (e.g., offer individual marketing programs), basic safety and security (e.g., make pedestrian and bicycle facility improvements and increase education and enforcement efforts), convenience and cost (e.g., institute higher-density, mixed land uses, and limited, more expensive automobile parking), enjoyment (e.g., plant street trees and increase awareness of non-motorized transportation benefits), and habit (e.g., target information about sustainable transportation options to people making key life changes). The components of the theory are supported by in-depth interview responses from the San Francisco Bay Area.},
  journal = {Transport Policy},
  url = {http://www.sciencedirect.com/science/article/pii/S0967070X12001643},
  author = {Schneider, R. J.},
  month = jan,
  year = {2013},
  pages = {128-137},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\LPB484XF\\Schneider - 2013 - Theory of routine mode choice decisions An operat.pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\SWJ8HX3Y\\S0967070X12001643.html}
}

@book{smith_ill_2011,
  title = {'{{I}}'ll Just Take the Car': Improving Bicycle Transportation to Encourage Its Use on Short Trips},
  isbn = {978-0-478-37125-3 978-0-478-37126-0},
  abstract = {This research, from July 2008 to June 2010, applied the affective design methodology to the goal of increasing practical cycling in New Zealand. A literature review revealed that overseas best practice is for integrated local cycling policies. Theories of diffusion of innovations and contemplation of change were highlighted and used to inform the project. A review of the New Zealand cycling market showed limited choice of and access to practical cycling tools. A survey of 234 New Zealand cyclists and non-cyclists demonstrated differences between the groups in perception of bicycles and cyclists, with more agreement for unfamiliar practical cyclists and bicycles. Practical workshops explored the effect of direct cycling experience on perceptions. A "practical cycling system design model" was proposed, along with recommendations for its implementation.},
  publisher = {{NZ Transport Agency}},
  url = {http://www.nzta.govt.nz/resources/research/reports/426/docs/426.pdf},
  author = {Smith, Paul and Wilson, Mike and Armstrong, Tim and {NZ Transport Agency}},
  year = {2011},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\HE2UEMP3\\Smith et al. - 2011 - 'I'll just take the car' improving bicycle transp.pdf},
  note = {OCLC: 705463743}
}

@misc{pjcci_pietons_2019,
  title = {{Pi{\'e}tons et cyclistes {$\star$} PJCCI}},
  abstract = {Sur cette page, les pi{\'e}tons et les cyclistes trouveront diverses informations sur les pistes qu'ils peuvent explorer, les points de vue {\`a} d{\'e}couvrir ainsi que la saison d'exploitation. Cette ann{\'e}e en grande nouveaut{\'e}, un nouvel affichage vous permet de savoir en un coup d'{\oe}il~si la piste est ouverte ou ferm{\'e}e. Projet de simulation d'exploitation hivernale \ldots{}},
  journal = {PJCCI},
  url = {https://jacquescartierchamplain.ca/circulation-travaux/pietons-et-cyclistes/},
  author = {PJCCI},
  year = {2019},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\HA3YBCZH\\Piste_multifonctionnelle_du_pont_Jacques-Cartier_simulation_exploitation_hivernale_2019-11-07.pdf;C\:\\Users\\vincent.le_falher\\Zotero\\storage\\7LIZRPUI\\pietons-et-cyclistes.html}
}

@misc{dustin_realtime_2019,
  title = {Realtime {{Semantic Segmentation}} on {{Jetson Nano}} in {{Python}} and {{C}}++},
  abstract = {Recently I updated the Hello AI World project on GitHub with new semantic segmentation models based on FCN-ResNet18 that run in realtime on Jetson Nano, in addition to Python bindings and examples for segmenting images and live camera video. Like with the other networks in the project, they use NVID},
  url = {https://www.linkedin.com/pulse/realtime-semantic-segmentation-jetson-nano-python-c-dustin-franklin},
  author = {Dustin, F.},
  month = oct,
  year = {2019},
  file = {C\:\\Users\\vincent.le_falher\\Zotero\\storage\\CPUFDUHW\\realtime-semantic-segmentation-jetson-nano-python-c-dustin-franklin.html}
}


