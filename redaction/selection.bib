
@article{shahian_jahromi_real-time_2019,
	title = {Real-{Time} {Hybrid} {Multi}-{Sensor} {Fusion} {Framework} for {Perception} in {Autonomous} {Vehicles}},
	doi = {10.3390/s19204357},
	abstract = {There are many sensor fusion frameworks proposed in the literature using different sensors and fusion methods combinations and configurations. More focus has been on improving the accuracy performance; however, the implementation feasibility of these frameworks in an autonomous vehicle is less explored. Some fusion architectures can perform very well in lab conditions using powerful computational resources; however, in real-world applications, they cannot be implemented in an embedded edge computer due to their high cost and computational need. We propose a new hybrid multi-sensor fusion pipeline configuration that performs environment perception for autonomous vehicles such as road segmentation, obstacle detection, and tracking. This fusion framework uses a proposed encoder-decoder based Fully Convolutional Neural Network (FCNx) and a traditional Extended Kalman Filter (EKF) nonlinear state estimator method. It also uses a configuration of camera, LiDAR, and radar sensors that are best suited for each fusion method. The goal of this hybrid framework is to provide a cost-effective, lightweight, modular, and robust (in case of a sensor failure) fusion system solution. It uses FCNx algorithm that improve road detection accuracy compared to benchmark models while maintaining real-time efficiency that can be used in an autonomous vehicle embedded computer. Tested on over 3K road scenes, our fusion algorithm shows better performance in various environment scenarios compared to baseline benchmark networks. Moreover, the algorithm is implemented in a vehicle and tested using actual sensor data collected from a vehicle, performing real-time environment perception.},
	journal = {Sensors (Basel, Switzerland)},
	author = {Shahian Jahromi, B. and Tulabandhula, T. and Cetin, S.},
	year = {2019},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\220\\Shahian Jahromi et al. - 2019 - Real-Time Hybrid Multi-Sensor Fusion Framework for.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\219\\display.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\WZBPETIC\\Shahian Jahromi et al. - 2019 - Real-Time Hybrid Multi-Sensor Fusion Framework for.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\DZH9FHZC\\display.html:text/html;Full Text:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\KJD5YSVG\\Shahian Jahromi et al. - 2019 - Real-Time Hybrid Multi-Sensor Fusion Framework for.pdf:application/pdf},
}

@article{fu_risk-based_2017,
	title = {A {Risk}-{Based} {Approach} to {Winter} {Road} {Surface} {Condition} {Classification}},
	doi = {10.1139/cjce-2016-0215},
	abstract = {This paper presents a risk-based approach for classifying the road surface conditions of a highway network under winter weather events. A relative risk index (RRI) is developed to capture the effec..., Cet article présente une approche fondée sur le risque afin de classifier les conditions de la surface de roulement d'un réseau d'autoroutes en fonction d'événements météorologiques d'hiver. On a d...},
	journal = {Canadian Journal of Civil Engineering},
	author = {Fu, Liping and Thakali, Lalita and Kwon, Tae J. and Usman, Taimur},
	month = mar,
	year = {2017},
	pages = {182--191},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\242\\Fu et al. - 2017 - A risk-based approach to winter road surface condi.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\241\\cjce-2016-0215.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\ADUI5UAT\\Fu et al. - 2017 - A risk-based approach to winter road surface condi.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\9KTBJ93I\\cjce-2016-0215.html:text/html;Full Text:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\I9DS42SR\\Fu et al. - 2017 - A Risk-Based Approach to Winter Road Surface Condi.pdf:application/pdf},
}

@article{cheng_road_2019,
	title = {Road {Surface} {Condition} {Classification} {Using} {Deep} {Learning}},
	doi = {10.1016/j.jvcir.2019.102638},
	abstract = {Traditional image recognition technology currently cannot achieve the fast real-time high-accuracy performance necessary for road recognition in intelligent driving. Deep learning models have been recently emerging as promising tools to achieve this performance. The recognition performance of such models can be boosted using appropriate selection of the activation functions. This paper proposes a deep learning approach for the classification of road surface conditions, and constructs a new activation function based on the rectified linear unit Rectified Linear Units (ReLu) activation function. The experimental results show a classification accuracy of 94.89\% on the road state database. Experiments on public datasets demonstrate that the proposed convolutional neural network model with the improved activation function has better generalization and excellent classification performance. © 2019 Elsevier Inc.},
	journal = {Journal of Visual Communication and Image Representation},
	author = {Cheng, L. and Zhang, X. and Shen, J.},
	year = {2019},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\273\\Cheng et al. - 2019 - Road surface condition classification using deep l.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\249\\display.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\4GXFUH25\\Cheng et al. - 2019 - Road surface condition classification using deep l.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\WY5MAEIW\\display.html:text/html},
}

@article{pan_winter_nodate,
	title = {Winter {Road} {Surface} {Condition} {Recognition} {Using} a {Pre}-{Trained} {Deep} {Convolutional} {Neural} {Network}},
	author = {Pan, Guangyuan and Fu, Liping and Yu, Ruifan and Muresan, Matthew},
	pages = {13},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\280\\Pan et al. - Winter Road Surface Condition Recognition Using a .pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\PLISZSFZ\\Pan et al. - Winter Road Surface Condition Recognition Using a .pdf:application/pdf},
}

@book{arai_advances_2019,
	title = {Advances in {Computer} {Vision}: {Proceedings} of the 2019 {Computer} {Vision} {Conference} ({CVC}), {Volume} 1},
	isbn = {978-3-030-17795-9},
	url = {http://ebookcentral.proquest.com/lib/usherbrookemgh-ebooks/detail.action?docID=5755789},
	abstract = {This book presents a remarkable collection of chapters covering a wide range of topics in the areas of Computer Vision, both from theoretical and application perspectives. It gathers the proceedings of the Computer Vision Conference (CVC 2019), held in Las Vegas, USA from May 2 to 3, 2019. The conference attracted a total of 371 submissions from pioneering researchers, scientists, industrial engineers, and students all around the world. These submissions underwent a double-blind peer review process, after which 120 (including 7 poster papers) were selected for inclusion in these proceedings. The book's goal is to reflect the intellectual breadth and depth of current research on computer vision, from classical to intelligent scope. Accordingly, its respective chapters address state-of-the-art intelligent methods and techniques for solving real-world problems, while also outlining future research directions. Topic areas covered include Machine Vision and Learning, Data Science, Image Processing, Deep Learning, and Computer Vision Applications.},
	publisher = {Springer},
	author = {Arai, Kohei and Kapoor, Supriya},
	year = {2019},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\295\\detail.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\402\\detail.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\7ICLRNCX\\detail.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\3YW3W9H8\\detail.html:text/html},
}

@article{lee_deep_2018,
	title = {A {Deep} {Neural} {Network} {Architecture} for {Real}-{Time} {Semantic} {Segmentation} on {Embedded} {Board}},
	doi = {10.5626/JOK.2018.45.1.94},
	abstract = {A Deep Neural Network Architecture for Real-Time Semantic Segmentation on Embedded Board deep learning;neural network;semantic segmentation;autonomous driving;embedded board; We propose Wide Inception ResNet (WIR Net) an optimized neural network architecture as a real-time semantic segmentation method for autonomous driving. The neural network architecture consists of an encoder that extracts features by applying a residual connection and inception module, and a decoder that increases the resolution by using transposed convolution and a low layer feature map. We also improved the performance by applying an ELU activation function and optimized the neural network by reducing the number of layers and increasing the number of filters. The performance evaluations used an NVIDIA Geforce GTX 1080 and TX1 boards to assess the class and category IoU for cityscapes data in the driving environment. The experimental results show that the accuracy of class IoU 53.4, category IoU 81.8 and the execution speed of \$640\{{\textbackslash}textbackslashtimes\}360\$, \$720\{{\textbackslash}textbackslashtimes\}480\$ resolution image processing 17.8fps and 13.0fps on TX1 board.},
	journal = {Journal of KIISE},
	author = {Lee, J. and Lee, Y.},
	year = {2018},
	pages = {94--98},
}

@inproceedings{pathak_architecturally_2019,
	title = {Architecturally {Compressed} {CNN}: {An} {Embedded} {Realtime} {Classifier} ({NXP} {Bluebox2}.0 with {RTMaps})},
	doi = {10.1109/CCWC.2019.8666495},
	abstract = {The convolution neural networks have revolutionized the computer vision domain. It has proven to be a dominant technology to carry out tasks such as image classification, semantic segmentation, and object detection. The convolution neural networks surpass the performance of the existing algorithms such as SIFT, HOG, etcetera. Where, instead of manually engineering the features, supervised learning help to learn the essential low-level and high-level features necessary for classifications. The convolution neural networks have become a popular tool to counter computer vision problems. However, it is computationally, and memory intensive to train and deploy the network because of the model size of a deep convolution neural networks. However, the research in the field of design space exploration (DSE) of neural networks and compression techniques to develop compact architectures, have made convolution neural networks memory and computationally efficient. These techniques have also improved the feasibility of convolution neural network for deployment on embedded targets. The paper explores the concept of compact convolution filters to reduce the number of parameters in a convolution neural network. The intuition behind the approach is that replacing convolution filters with a stack of compact convolution filters helps in developing a compact architecture with competitive accuracy. This paper explores the fire module a compact convolution filter and proposes a method of recreating a state-of-the-art architecture VGG-16 using the fire modules to develop a compact architecture, which is further trained on the CIFAR-10 dataset and deployed on a real-time embedded platform known as Bluebox 2.0 by NXP using RTMaps software framework.},
	booktitle = {2019 {IEEE} 9th {Annual} {Computing} and {Communication} {Workshop} and {Conference} ({CCWC})},
	author = {Pathak, D. and El-Sharkawy, M.},
	month = jan,
	year = {2019},
	pages = {0331--0336},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\305\\8666495.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\Z47CBT6U\\8666495.html:text/html},
}

@article{abouzahir_iot-empowered_2017,
	title = {{IoT}-{Empowered} {Smart} {Agriculture}: {A} {Real}-{Time} {Light}-{Weight} {Embedded} {Segmentation} {System}},
	doi = {10.1007/978-3-319-68179-5_28},
	abstract = {Internet of Things (IoT) is an emerging technology where standalone equipments and autonomous devices are connected to each other and users via Internet. When IoT concept meets agriculture, the future of farming is pushed to the next level, giving birth to what is called “Smart Agriculture” or “Precision Agriculture”. The most important benefit from IoT is that a user can daily monitor his crop online in a seamless fashion. High quality data gathered from various sensors and transferred wirelessly to farm database will increase farmers understanding to their landuse leading to increasing income and product quality. One of the monitoring process is weeds detection and crop yield estimation using camera sensors. The acquired images help farmers to build map of weeds distribution or yield quantity all over the field, these maps can be used either for real-time processing or to predetermine weeds regions based on field maps history of the previous seasons. This process is referred to as segmentation problem. Several algorithms have been proposed for that purpose, however, these algorithms were run only on high performance computers. In this paper, we evaluate performance and the robustness of the most used legacy algorithms under local conditions. We focused on implementing these schemes within real-time application constraint. For instance, these algorithms were implemented and run in a low-cost embedded system. © Springer International Publishing AG 2017.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Abouzahir, S. and Sadik, M. and Sabir, E.},
	year = {2017},
	pages = {319--332},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\308\\display.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\EMY6PJB8\\display.html:text/html},
}

@article{gilman_real-time_2014,
	title = {Real-{Time} {Segmentation} for {Baggage} {Tracking} on a {Cost} {Effective} {Embedded} {Platform}},
	doi = {10.1504/IJISTA.2014.068821},
	abstract = {This paper describes segmentation and tracking parts of a machine vision based airport baggage tracking system. A simplified codebook based background subtraction method is used to segment the bag from a semi-static background. Morphological processing using an integral image is used to filter the foreground mask and the bag location is found using statistical methods. The system was implemented on a cost effective embedded processor and runs in real time at 30 fps. Five ARM based embedded platforms are evaluated and it is shown that all of them are capable of the required performance. Copyright © 2014 Inderscience Enterprises Ltd.},
	journal = {International Journal of Intelligent Systems Technologies and Applications},
	author = {Gilman, A. and Johnson, M.},
	year = {2014},
	pages = {245--257},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\313\\display.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\WKT6KCFF\\display.html:text/html},
}

@article{zhang_real-time_2012,
	title = {Real-{Time} {Embedded} {Implementation} of {Foreground}/{Background} {Segmentation} {Algorithm}},
	abstract = {Foreground/Background (FG/BG) segmentation is one of the most widely-used computer vision (CV) algorithms. Its basic algorithm takes advantages of correlation between background of different pixels or frames, predicts gray intensity for every pixel point and judges whether it belongs to foreground or background based on the prediction and actual value. Several FG/BG algorithms used in different situation were introduced firstly. Considering that those computationally expensive algorithms are hard to operate in real time using the traditional processor-based platform, its real-time embedded implementation was highlighted after this algorithm being verified. The design methodology and flow of reconfigurable computing (RC) which was an advanced method to implement those algorithms were introduced. In addition, several important issues about hardware implementation were discussed. A case of study to elaborate how it works was presented, and how to attach the implemented algorithm into the SoPC-based imaging system to achieve an integrated system was described.},
	journal = {Hongwai yu Jiguang Gongcheng/Infrared and Laser Engineering},
	author = {Zhang, H. and Chang, Y. and Li, F. and Shen, Y.},
	year = {2012},
	pages = {523--530},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\316\\display.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\3MQVZEBH\\display.html:text/html},
}

@inproceedings{chougule_efficient_2018,
	title = {An {Efficient} {Encoder}-{Decoder} {CNN} {Architecture} for {Reliable} {Multilane} {Detection} in {Real} {Time}},
	doi = {10.1109/IVS.2018.8500598},
	abstract = {Multilane detection system is a vital prerequisite for realizing higher ADAS functionality of autonomous navigation. In this work, we present an efficient convolutional neural network (CNN) architecture for real time detection of multiple lane boundaries using a camera sensor. Our network has a simple encoder-decoder architecture and is a special two class semantic segmentation network designed to segment lane boundaries. Efficacy of our network stems from two key insights which are at the foundation of all our design decisions. Firstly, we term a lane boundary as a weak class object in the context of semantic segmentation. We show that the weak class objects which occupy relatively few pixels in the scene, also have a relatively low detection accuracy among the know segmentation methods. We present novel design choices and intuitions to improve the segmentation accuracy of weak class objects, which in turn reduces computation time. Our second insight lies in the manner we depict the ground truth information in our derived dataset. Instead of annotating just the visible lane markers, we accurately delineate the lane boundaries in the ground truth for challenging scenarios like occlusions, low light and degraded lane markings. We then leverage the CNN's ability to concisely summarize the global and local context in an image, for accurately inferring lane boundaries in these challenging cases. We evaluate our network against ENet and FCN-8, and found it performing notably better in terms of speed and accuracy. Our network achieves an encouraging 46 FPS performance on NVIDIA Drive PX2 platform and it has been validated on our test vehicle in highway driving conditions. © 2018 IEEE.},
	booktitle = {{IEEE} {Intelligent} {Vehicles} {Symposium}, {Proceedings}},
	author = {Chougule, S. and Ismail, A. and Soni, A. and Kozonek, N. and Narayan, V. and Schulze, M.},
	year = {2018},
	pages = {1444--1451},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\323\\display.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\4GRC47QT\\display.html:text/html},
}

@book{copel_whats_2016,
	title = {What's the {Difference} {Between} {Deep} {Learning} {Training} and {Inference}?},
	url = {https://blogs.nvidia.com/blog/2016/08/22/difference-deep-learning-training-inference-ai/},
	abstract = {Let's break let's break down the progression from deep-learning training to inference in the context of AI how they both function.},
	author = {Copel, M.},
	month = aug,
	year = {2016},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\327\\difference-deep-learning-training-inference-ai.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\408\\difference-deep-learning-training-inference-ai.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\QUZFSTMJ\\difference-deep-learning-training-inference-ai.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\IESYZXHJ\\difference-deep-learning-training-inference-ai.html:text/html},
}

@article{chong_real-time_1992,
	title = {Real-{Time} {Edge} {Detection} and {Image} {Segmentation}},
	doi = {10.1007/BF00142412},
	abstract = {The implementation of real-time edge detection and image segmentation using analog VLSI is described. A novel technique of image segmentation using radially propagating signals is discussed. Current-mode processing is used to avoid offset voltages and lead to circuit implementations which are compatible with standard CMOS processes. A system using raylike signal propagation and implemented using 3μ-CMOS technology is described together with experimental results. © 1992 Kluwer Academic Publishers.},
	journal = {Analog Integrated Circuits and Signal Processing},
	author = {Chong, C. P. and Salama, C. A. T. and Smith, K. C.},
	year = {1992},
	pages = {117--130},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\330\\display.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\9NNA52GL\\display.html:text/html},
}

@article{bernas_edge_2017,
	title = {Edge {Real}-{Time} {Medical} {Data} {Segmentation} for {IoT} {Devices} with {Computational} and {Memory} {Constrains}},
	doi = {10.1007/978-3-319-67077-5_12},
	abstract = {The Internet of Things (IoT) becomes very important tool for data gathering and management in many environments. The majority of dedicated solutions register data only at time of events, while in case of medical data full records for long time periods are usually needed. The precision of acquired data and the amount of data sent by sensor-equipped IoT devices has vital impact on lifetime of these devices. In case of solutions, where multiple sensors are available for single device with limited computation power and memory, the complex compression or transformation methods cannot be applied - especially in case of nano device injected to a body. Thus this paper is focused on linear complexity segmentation algorithms that can be used by the resource-limited devices. The state-of-art data segmentation methods are analysed and adapted for simple IoT devices. Two segmentation algorithms are proposed and tested on a real-world dataset collected from a prototype of the IoT device. © 2017, Springer International Publishing AG.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Bernas, M. and P{\textbackslash}laczek, B. and Sapek, A.},
	year = {2017},
	pages = {119--128},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\333\\display.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\NSCHUQ8F\\display.html:text/html},
}

@article{blanco-filgueira_deep_2019,
	title = {Deep {Learning}-{Based} {Multiple} {Object} {Visual} {Tracking} on {Embedded} {System} for {IoT} and {Mobile} {Edge} {Computing} {Applications}},
	doi = {10.1109/JIOT.2019.2902141},
	abstract = {Compute and memory demands of state-of-the-art deep learning methods are still a shortcoming that must be addressed to make them useful at Internet of Things (IoT) end-nodes. In particular, recent results depict a hopeful prospect for image processing using convolutional neural networks, CNNs, but the gap between software and hardware implementations is already considerable for IoT and mobile edge computing applications due to their high power consumption. This proposal performs low-power and real time deep learning-based multiple object visual tracking implemented on an NVIDIA Jetson TX2 development kit. It includes a camera and wireless connection capability and it is battery powered for mobile and outdoor applications. A collection of representative sequences captured with the on-board camera, dETRUSC video dataset, is used to exemplify the performance of the proposed algorithm and to facilitate benchmarking. The results in terms of power consumption and frame rate demonstrate the feasibility of deep learning algorithms on embedded platforms although more effort in the joint algorithm and hardware design of CNNs is needed.},
	journal = {IEEE Internet of Things Journal},
	author = {Blanco-Filgueira, B. and García-Lesta, D. and Fernández-Sanjurjo, M. and Brea, V. M. and López, M.},
	month = jun,
	year = {2019},
	pages = {5423--5431},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\337\\Blanco-Filgueira et al. - 2019 - Deep Learning-Based Multiple Object Visual Trackin.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\338\\8653851.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\DHUE3NLF\\Blanco-Filgueira et al. - 2019 - Deep Learning-Based Multiple Object Visual Trackin.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\GVFA5GF7\\8653851.html:text/html;Submitted Version:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\BEN5JNZJ\\Blanco-Filgueira et al. - 2019 - Deep Learning-Based Multiple Object Visual Trackin.pdf:application/pdf},
}

@article{nguyen_mavnet_2019,
	title = {{MAVNet}: {An} {Effective} {Semantic} {Segmentation} {Micro}-{Network} for {MAV}-{Based} {Tasks}},
	url = {http://arxiv.org/abs/1904.01795},
	abstract = {Real-time semantic image segmentation on platforms subject to size, weight and power (SWaP) constraints is a key area of interest for air surveillance and inspection. In this work, we propose MAVNet: a small, light-weight, deep neural network for real-time semantic segmentation on micro Aerial Vehicles (MAVs). MAVNet, inspired by ERFNet [1], features 400 times fewer parameters and achieves comparable performance with some reference models in empirical experiments. Our model achieves a trade-off between speed and accuracy, achieving up to 48 FPS on an NVIDIA 1080Ti and 9 FPS on the NVIDIA Jetson Xavier when processing high resolution imagery. Additionally, we provide two novel datasets that represent challenges in semantic segmentation for real-time MAV tracking and infrastructure inspection tasks and verify MAVNet on these datasets. Our algorithm and datasets are made publicly available.},
	journal = {arXiv:1904.01795 [cs]},
	author = {Nguyen, T. and Shivakumar, S. S. and Miller, I. D. and Keller, J. and Lee, E. S. and Zhou, Alex and Ozaslan, Tolga and Loianno, Giuseppe and Harwood, Joseph H. and Wozencraft, Jennifer and Taylor, Camillo J. and Kumar, Vijay},
	month = jun,
	year = {2019},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\341\\Nguyen et al. - 2019 - MAVNet an Effective Semantic Segmentation Micro-N.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\340\\1904.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\APHG29K3\\Nguyen et al. - 2019 - MAVNet an Effective Semantic Segmentation Micro-N.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\S2UXRTI4\\1904.html:text/html;Full Text:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\2BE89LRB\\Nguyen et al. - 2019 - MAVNet An Effective Semantic Segmentation Micro-N.pdf:application/pdf},
}

@article{orsic_defense_2019,
	title = {In {Defense} of {Pre}-{Trained} {ImageNet} {Architectures} for {Real}-{Time} {Semantic} {Segmentation} of {Road}-{Driving} {Images}},
	url = {http://arxiv.org/abs/1903.08469},
	abstract = {Recent success of semantic segmentation approaches on demanding road driving datasets has spurred interest in many related application fields. Many of these applications involve real-time prediction on mobile platforms such as cars, drones and various kinds of robots. Real-time setup is challenging due to extraordinary computational complexity involved. Many previous works address the challenge with custom lightweight architectures which decrease computational complexity by reducing depth, width and layer capacity with respect to general purpose architectures. We propose an alternative approach which achieves a significantly better performance across a wide range of computing budgets. First, we rely on a light-weight general purpose architecture as the main recognition engine. Then, we leverage light-weight upsampling with lateral connections as the most cost-effective solution to restore the prediction resolution. Finally, we propose to enlarge the receptive field by fusing shared features at multiple resolutions in a novel fashion. Experiments on several road driving datasets show a substantial advantage of the proposed approach, either with ImageNet pre-trained parameters or when we learn from scratch. Our Cityscapes test submission entitled SwiftNetRN-18 delivers 75.5\% MIoU and achieves 39.9 Hz on 1024×2048 images on GTX1080Ti.},
	journal = {arXiv:1903.08469 [cs]},
	author = {Oršić, M. and Krešo, I. and Bevandić, P. and Šegvić, S.},
	month = apr,
	year = {2019},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\348\\Oršić et al. - 2019 - In Defense of Pre-trained ImageNet Architectures f.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\XXINUYDU\\Oršić et al. - 2019 - In Defense of Pre-trained ImageNet Architectures f.pdf:application/pdf;Full Text:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\TP4G3FXD\\Oršić et al. - 2019 - In Defense of Pre-Trained ImageNet Architectures f.pdf:application/pdf},
}

@article{spek_cream_2018,
	title = {{CReaM}: {Condensed} {Real}-{Time} {Models} for {Depth} {Prediction} {Using} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1807.08931},
	abstract = {Since the resurgence of CNNs the robotic vision community has developed a range of algorithms that perform classification, semantic segmentation and structure prediction (depths, normals, surface curvature) using neural networks. While some of these models achieve state-of-the art results and super human level performance, deploying these models in a time critical robotic environment remains an ongoing challenge. Real-time frameworks are of paramount importance to build a robotic society where humans and robots integrate seamlessly. To this end, we present a novel real-time structure prediction framework that predicts depth at 30 frames per second on an NVIDIA-TX2. At the time of writing, this is the first piece of work to showcase such a capability on a mobile platform. We also demonstrate with extensive experiments that neural networks with very large model capacities can be leveraged in order to train accurate condensed model architectures in a “from teacher to student” style knowledge transfer.},
	journal = {arXiv:1807.08931 [cs]},
	author = {Spek, A. and Dharmasiri, T. and Drummond, T.},
	month = jul,
	year = {2018},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\351\\Spek et al. - 2018 - CReaM Condensed Real-time Models for Depth Predic.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\N4ENC8A2\\Spek et al. - 2018 - CReaM Condensed Real-time Models for Depth Predic.pdf:application/pdf;Full Text:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\9IYHJ4I5\\Spek et al. - 2018 - CReaM Condensed Real-Time Models for Depth Predic.pdf:application/pdf},
}

@book{dettmers_deep_2015,
	title = {Deep {Learning} in a {Nutshell}: {History} and {Training}},
	url = {https://devblogs.nvidia.com/deep-learning-nutshell-history-training/},
	abstract = {Part 2 of an intuitive and gentle introduction to deep learning. Covers the most important deep learning concepts, giving an understanding rather than mathematical and theoretical details.},
	author = {Dettmers, T.},
	month = dec,
	year = {2015},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\354\\deep-learning-nutshell-history-training.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\YH7SGLKB\\deep-learning-nutshell-history-training.html:text/html},
}

@book{jiaconda_concise_2019,
	title = {A {Concise} {History} of {Neural} {Networks}},
	url = {https://towardsdatascience.com/a-concise-history-of-neural-networks-2070655d3fec},
	abstract = {“From the barren landscapes inside our personal devices come furtive anthems hummed by those digital servants who will one day be our{\textbackslash}ldots},
	author = {{Jiaconda}},
	month = apr,
	year = {2019},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\356\\a-concise-history-of-neural-networks-2070655d3fec.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\FSXP52Q9\\a-concise-history-of-neural-networks-2070655d3fec.html:text/html},
}

@book{kurenkov_brief_2015,
	title = {A '{Brief}' {History} of {Neural} {Nets} and {Deep} {Learning}},
	url = {https://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning/},
	abstract = {The beginning of a story spanning half a century, about how we learned to make computers learn},
	author = {Kurenkov, A.},
	year = {2015},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\358\\a-brief-history-of-neural-nets-and-deep-learning.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\TDM6CSC2\\a-brief-history-of-neural-nets-and-deep-learning.html:text/html},
}

@book{beam_deep_2017,
	title = {Deep {Learning} 101 - {Part} 1: {History} and {Background}},
	url = {https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html},
	author = {Beam, A.},
	year = {2017},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\360\\deep_learning_101_part1.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\396\\deep-learning-101.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\A76R9M4T\\deep_learning_101_part1.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\B2GWDPLR\\deep-learning-101.html:text/html},
}

@incollection{cios_deep_2018,
	title = {Deep {Neural} {Networks}—{A} {Brief} {History}},
	isbn = {978-3-319-67945-7 978-3-319-67946-4},
	booktitle = {Advances in {Data} {Analysis} with {Computational} {Intelligence} {Methods}},
	publisher = {Springer International Publishing},
	author = {Cios, K. J.},
	editor = {Gawęda, A. E. and Kacprzyk, J. and Rutkowski, L. and Yen, G. G.},
	year = {2018},
	doi = {10.1007/978-3-319-67946-4_7},
	pages = {183--200},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\361\\Cios - 2018 - Deep Neural Networks—A Brief History.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\X69NXS6I\\Cios - 2018 - Deep Neural Networks—A Brief History.pdf:application/pdf;Submitted Version:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\WMR9FI7J\\Cios - 2018 - Deep Neural Networks—A Brief History.pdf:application/pdf},
}

@article{alom_history_2018,
	title = {The {History} {Began} from {AlexNet}: {A} {Comprehensive} {Survey} on {Deep} {Learning} {Approaches}},
	abstract = {In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly, and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bio-informatics, natural language processing (NLP), cybersecurity, and many others.},
	author = {Alom, Z. and Taha, T. M. and Yakopcic, C. and Westberg, S. and Sidike, P. and Nasrin, M. S.},
	year = {2018},
	pages = {39},
	file = {Alom et al. - The History Began from AlexNet A Comprehensive Su.pdf:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\84EXBIY7\\Alom et al. - The History Began from AlexNet A Comprehensive Su.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\363\\Alom et al. - The History Began from AlexNet A Comprehensive Su.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\457\\Alom et al. - 2018 - The History Began from AlexNet A Comprehensive Su.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\461\\Alom et al. - 2018 - The History Began from AlexNet A Comprehensive Su.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\443\\1803.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\458\\1803.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\462\\1803.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\FW6ANCUN\\Alom et al. - The History Began from AlexNet A Comprehensive Su.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\3IB8QVUF\\Alom et al. - 2018 - The History Began from AlexNet A Comprehensive Su.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\IJSXYLYT\\Alom et al. - 2018 - The History Began from AlexNet A Comprehensive Su.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\IAEVL4YF\\1803.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\C9IZZT5M\\1803.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\HDBVKHSY\\1803.html:text/html},
}

@article{nielsen_neural_2015,
	title = {Neural {Networks} and {Deep} {Learning}},
	url = {http://neuralnetworksanddeeplearning.com},
	author = {Nielsen, M. A.},
	year = {2015},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\366\\neuralnetworksanddeeplearning.com.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\385\\index.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\TKEMP2L4\\neuralnetworksanddeeplearning.com.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\LJ3DGJK4\\index.html:text/html},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	url = {http://www.deeplearningbook.org/},
	author = {Goodfellow, I. and Bengio, Y. and Courville, A.},
	year = {2016},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\436\\Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT).pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\383\\www.deeplearningbook.org.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\8SRGXH8N\\Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT).pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\LGDSYFW6\\www.deeplearningbook.org.html:text/html},
}

@book{nvidia_nvidia_2015,
	title = {{NVIDIA} {Deep} {Learning}, {AI}, \& {HPC} {Classes} \& {Workshops}},
	url = {https://www.nvidia.com/en-us/deep-learning-ai/education/},
	abstract = {Find hands-on AI and Accelerated Computing courses and events.},
	author = {{NVIDIA}},
	year = {2015},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\377\\education.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\MX77BRXZ\\education.html:text/html},
}

@book{nvidia_nvidia_2015-1,
	title = {{NVIDIA} {Teaching} {Kits}},
	url = {https://developer.nvidia.com/teaching-kits},
	abstract = {NVIDIA Teaching Kits are complete course solutions for use by educators in a variety of academic disciplines that benefit from GPU-accelerated computing. Co-developed with leading university faculty, Teaching Kits provide full curriculum design coupled with ease-of-use. Educators can bridge academic theory with real-world application to empower next-generation innovators with critical AI skillsets.},
	author = {{NVIDIA}},
	month = nov,
	year = {2015},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\379\\teaching-kits.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\LFB2Z7YM\\teaching-kits.html:text/html},
}

@book{bishop_pattern_2006,
	series = {Information {Science} and {Statistics}},
	title = {Pattern {Recognition} and {Machine} {Learning}},
	isbn = {978-0-387-31073-2},
	publisher = {Springer},
	author = {Bishop, C. M.},
	year = {2006},
	lccn = {Q327 .B52 2006},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\380\\Bishop - 2006 - Pattern recognition and machine learning.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\5QGYD8UC\\Bishop - 2006 - Pattern recognition and machine learning.pdf:application/pdf},
}

@book{pytorchcom_welcome_2017,
	title = {Welcome to {PyTorch} {Tutorials} — {PyTorch} {Tutorials} 1.3.0 {Documentation}},
	url = {https://pytorch.org/tutorials/},
	author = {{PyTorch.com}},
	year = {2017},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\387\\tutorials.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\H8NXJ7GX\\tutorials.html:text/html},
}

@book{sharma_history_2019,
	series = {Intelligent {Systems} {Reference} {Library}},
	title = {The {History}, {Present} and {Future} with {Iot}},
	isbn = {18684394 (ISSN)},
	abstract = {Human beings quest for making comfortable life is due to their inquisitiveness about technical arena. Over the last few decades, mankind had experienced technical transformational journey with the inventions of new technology frontiers. These frontiers have interacted with human beings and performed every possible work in shorter period of time and with a much greater accuracy. With the advent of `Smart Concepts', the world is now becoming more connected. Precisely termed as hyper-connected world. The smart concepts includes smart phones, smart devices, smart applications and smart cities. These smarter concepts forms an ecosystem of devices whose basic work is to connect various devices to send and receive data. Internet of Things is one the dominating technology that keeps eye on the connected smart devices. Internet of Things has bought applications from fiction to fact enabling fourth industrial revolution. It has laid an incredible impact on the technical, social, economic and on the lives of human and machines. Scientists claim that the potential benefit derived from this technology will sprout a foreseeable future where the smart objects sense, think and act. Internet of Things is the trending technology and embodies various concepts such as fog computing, edge computing, communication protocols, electronic devices, sensors, geo-location etc. The chapter presents the comprehensive information about the evolution of Internet of Things, its present developments to its futuristic applications. © Springer Nature Switzerland AG 2019.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Sharma, N. and Shamkuwar, M. and Singh, I.},
	year = {2019},
	doi = {10.1007/978-3-030-04203-5_3},
	keywords = {Communication model, Edge computing, Fog computing, Future of IoT, Internet of things, IoT, IoT applications, IoT architecture, IoT definition, IoT evolution, IoT history, IoT technologies, IoT trends, Sensors},
}

@book{nvidia_jetson_2019,
	title = {Jetson {Nano}},
	url = {https://developer.nvidia.com/embedded/jetson-nano},
	abstract = {Jetson Nano is a small, powerful computer for embedded applications and AI IoT that delivers the power of modern AI in a \$129 module. Get started fast with the comprehensive JetPack SDK with accelerated libraries for deep learning, computer vision, graphics, multimedia, and more.},
	author = {{NVIDIA}},
	month = mar,
	year = {2019},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\398\\jetson-nano.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\2ZVYHLTF\\jetson-nano.html:text/html},
}

@inproceedings{mody_low_2018,
	title = {Low {Cost} and {Power} {CNN}/{Deep} {Learning} {Solution} for {Automated} {Driving}},
	doi = {10.1109/ISQED.2018.8357325},
	abstract = {Automated driving functions, like highway driving and parking assist, are increasingly getting deployed in high-end cars with the ultimate goal of realizing self-driving car using Deep learning techniques like convolution neural network (CNN). For mass-market deployment, the embedded solution is required to address the right cost and performance envelope along with security and safety. In the case of automated driving, one of the key functionality is 'finding drivable free space', which is addressed using deep learning techniques like CNN. These CNN networks pose huge computing requirements in terms of hundreds of GOPS/TOPS (Giga or Tera operations per second), which seems beyond the capability of today's embedded SoC. This paper covers various techniques consisting of fixed-point conversion, sparse multiplication, fusing of layers and network pruning, for tailoring on the embedded solution. These techniques are implemented on the device by means of optimized Deep learning library for inference. The paper concludes by demonstrating the results of a CNN network running in real time on TI's TDA2X embedded platform producing a high-quality drivable space output for automated driving. © 2018 IEEE.},
	booktitle = {Proceedings - {International} {Symposium} on {Quality} {Electronic} {Design}, {ISQED}},
	author = {Mody, M. and Kumar, D. and Swami, P. and Mathew, M. and Nagori, S.},
	year = {2018},
	pages = {432--436},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\406\\display.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\G6BHRUL3\\display.html:text/html},
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, K. and Zisserman, A.},
	month = apr,
	year = {2015},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\409\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\YERCKGBC\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;Full Text:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\UZSAIGST\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf},
}

@book{koh_model_2018,
	title = {Model {Zoo} - {Deep} {Learning} {Code} and {Pretrained} {Models} for {Transfer} {Learning}, {Educational} {Purposes}, and {More}},
	url = {https://modelzoo.co/},
	author = {Koh, J. Y.},
	year = {2018},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\412\\modelzoo.co.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\PDXXYJQH\\modelzoo.co.html:text/html},
}

@book{nvidia_jetson_2019-1,
	title = {Jetson {Nano}: {Deep} {Learning} {Inference} {Benchmarks}},
	url = {https://developer.nvidia.com/embedded/jetson-nano-dl-inference-benchmarks},
	abstract = {Jetson Nano can run a wide variety of advanced networks, including the full native versions of popular ML frameworks like TensorFlow, PyTorch, Caffe/Caffe2, Keras, MXNet, and others. These networks can be used to build autonomous machines and complex AI systems by implementing robust capabilities such as image recognition, object detection and localization, pose estimation, semantic segmentation, video enhancement, and intelligent analytics. To run the following benchmarks on your Jetson Nano, please see the instructions here.},
	author = {{NVIDIA}},
	month = apr,
	year = {2019},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\414\\jetson-nano-dl-inference-benchmarks.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\ALX5JPXA\\jetson-nano-dl-inference-benchmarks.html:text/html},
}

@inproceedings{mountelos_vehicle_2019,
	title = {Vehicle {Windshield} {Detection} by {Fast} and {Compact} {Encoder}-{Decoder} {FCN} {Architecture}},
	doi = {10.1109/MOCAST.2019.8741770},
	abstract = {Vehicle semantic segmentation is critical in many advanced driving assistance systems, traffic management, and security surveillance systems. Most of such systems are deployed on low computational embedded systems located in the vehicles or in remote gantry and roadside poles. While fully convolutional networks have been proved to be a powerful classifier being able to make inference on every single pixel of the input image, they entail high computational costs even for the inference process. In this paper, a vehicle windshield semantic segmentation is proposed utilizing a fast and compact encoder-decoder architecture of a fully convolutional network implemented in a low-power embedded system. The performed qualitative and quantitative performance measurements exemplify a real-time portable embedded solution which is competitive in terms of performance and inference time.},
	booktitle = {2019 8th {International} {Conference} on {Modern} {Circuits} and {Systems} {Technologies} ({MOCAST})},
	author = {Mountelos, A. and Amanatiadis, A. and Sirakoulis, G. and Kosmatopoulos, E. B.},
	month = may,
	year = {2019},
	pages = {1--4},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\416\\Mountelos et al. - 2019 - Vehicle Windshield Detection by Fast and Compact E.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\417\\8741770.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\2PJK783R\\Mountelos et al. - 2019 - Vehicle Windshield Detection by Fast and Compact E.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\K4P2X6V3\\8741770.html:text/html},
}

@inproceedings{bidyanta_real-time_2018,
	title = {Real-{Time} {GPU} {Based} {Video} {Segmentation} with {Depth} {Information}},
	doi = {10.1109/AICCSA.2018.8612854},
	abstract = {In the context of video segmentation with depth sensor, prior work maps the Metropolis algorithm, a simulated annealing based key routine during segmentation, onto an Nvidia Graphics Processing Unit (GPU) and achieves real-time performance for 320×256 video sequences. However that work utilizes depth information in a very limited manner. This paper presents a new GPU-based method that expands the use of depth information during segmentation and shows the improved segmentation quality over the prior work. In particular, we discuss various ways to restructure the segmentation flow, and evaluate the impact of several design choices on throughput and quality. We introduce a scaling factor for amplifying the interaction strength between two spatially neighboring pixels and increasing the clarity of borderlines. This allows us to reduce the number of required Metropolis iterations by over 50\% with the drawback of over-segmentation. We evaluate two design choices to overcome this problem. First, we incorporate depth information into the perceived color difference calculations between two pixels, and show that the interaction strengths between neighboring pixels can be more accurately modeled by incorporating depth information. Second, we pre-process the frames with Bilateral filter instead of Gaussian filter, and show its effectiveness in terms of reducing the difference between similar colors. Both approaches help improve the quality of the segmentation, and the reduction in Metropolis iterations helps improve the throughout from 29 fps to 34 fps for 320×256 video sequences.},
	booktitle = {2018 {IEEE}/{ACS} 15th {International} {Conference} on {Computer} {Systems} and {Applications} ({AICCSA})},
	author = {Bidyanta, N. and Akoglu, A.},
	month = oct,
	year = {2018},
	pages = {1--8},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\420\\Bidyanta and Akoglu - 2018 - Real-Time GPU Based Video Segmentation with Depth .pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\421\\8612854.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\SFYLHB76\\Bidyanta and Akoglu - 2018 - Real-Time GPU Based Video Segmentation with Depth .pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\I24RKF9K\\8612854.html:text/html},
}

@article{paszke_enet_2016,
	title = {{ENet}: {A} {Deep} {Neural} {Network} {Architecture} for {Real}-{Time} {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1606.02147},
	abstract = {The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18× faster, requires 75× less FLOPs, has 79× less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.},
	journal = {arXiv:1606.02147 [cs]},
	author = {Paszke, A. and Chaurasia, A. and Kim, S. and Culurciello, E.},
	month = jun,
	year = {2016},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\424\\Paszke et al. - 2016 - ENet A Deep Neural Network Architecture for Real-.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\88PMWQ63\\Paszke et al. - 2016 - ENet A Deep Neural Network Architecture for Real-.pdf:application/pdf;Full Text:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\PMUPXQUK\\Paszke et al. - 2016 - ENet A Deep Neural Network Architecture for Real-.pdf:application/pdf},
}

@inproceedings{runz_co-fusion_2017,
	title = {Co-{Fusion}: {Real}-{Time} {Segmentation}, {Tracking} and {Fusion} of {Multiple} {Objects}},
	isbn = {978-1-5090-4633-1},
	doi = {10.1109/ICRA.2017.7989518},
	abstract = {In this paper we introduce Co-Fusion, a dense SLAM system that takes a live stream of RGB-D images as input and segments the scene into different objects (using either motion or semantic cues) while simultaneously tracking and reconstructing their 3D shape in real time. We use a multiple model fitting approach where each object can move independently from the background and still be effectively tracked and its shape fused over time using only the information from pixels associated with that object label. Previous attempts to deal with dynamic scenes have typically considered moving regions as outliers, and consequently do not model their shape or track their motion over time. In contrast, we enable the robot to maintain 3D models for each of the segmented objects and to improve them over time through fusion. As a result, our system can enable a robot to maintain a scene description at the object level which has the potential to allow interactions with its working environment; even in the case of dynamic scenes.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Runz, M. and Agapito, L.},
	month = may,
	year = {2017},
	pages = {4471--4478},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\428\\Runz and Agapito - 2017 - Co-fusion Real-time segmentation, tracking and fu.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\YLEBY9A4\\Runz and Agapito - 2017 - Co-fusion Real-time segmentation, tracking and fu.pdf:application/pdf},
}

@inproceedings{long_fully_2015,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	doi = {10.1109/CVPR.2015.7298965},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Long, J. and Shelhamer, E. and Darrell, T.},
	month = jun,
	year = {2015},
	pages = {3431--3440},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\446\\Long et al. - 2015 - Fully convolutional networks for semantic segmenta.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\447\\7298965.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\2W3ELMUR\\Long et al. - 2015 - Fully convolutional networks for semantic segmenta.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\WE8SR4G7\\7298965.html:text/html;Submitted Version:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\UGWTM6IC\\Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf},
}

@article{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	url = {http://arxiv.org/abs/1506.02640},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	journal = {arXiv:1506.02640 [cs]},
	author = {Redmon, J. and Divvala, S. and Girshick, R. and Farhadi, A.},
	month = may,
	year = {2016},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\449\\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\286\\yolo.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\450\\1506.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\PPB74W3Q\\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\4FYCP55I\\yolo.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\8WYXNUAN\\1506.html:text/html;Full Text:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\6CBYKMP2\\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf},
}

@book{ramsundar_tensorflow_2018,
	edition = {1st},
	title = {{TensorFlow} for {Deep} {Learning}: {From} {Linear} {Regression} to {Reinforcement} {Learning}},
	isbn = {978-1-4919-8045-3},
	abstract = {Learn how to solve challenging machine learning problems with TensorFlow, Googles revolutionary new software library for deep learning. If you have some background in basic linear algebra and calculus, this practical book introduces machine-learning fundamentals by showing you how to design systems capable of detecting objects in images, understanding text, analyzing video, and predicting the properties of potential medicines. TensorFlow for Deep Learning teaches concepts through practical examples and helps you build knowledge of deep learning foundations from the ground up. Its ideal for practicing developers with experience designing software systems, and useful for scientists and other professionals familiar with scripting but not necessarily with designing learning algorithms. Learn TensorFlow fundamentals, including how to perform basic computation Build simple learning systems to understand their mathematical foundations Dive into fully connected deep networks used in thousands of applications Turn prototypes into high-quality models with hyperparameter optimization Process images with convolutional neural networks Handle natural language datasets with recurrent neural networks Use reinforcement learning to solve games such as tic-tac-toe Train deep networks with hardware including GPUs and tensor processing units},
	publisher = {O'Reilly Media, Inc.},
	author = {Ramsundar, Bharath and Zadeh, Reza Bosagh},
	year = {2018},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\194\\Zadeh and Ramsundar - TensorFlow for Deep Learning.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\H33XR5HZ\\Zadeh and Ramsundar - TensorFlow for Deep Learning.pdf:application/pdf},
}

@article{edge_exploring_2018,
	title = {Exploring {E}-{Bikes} as a {Mode} of {Sustainable} {Transport}: {A} {Temporal} {Qualitative} {Study} of the {Perspectives} of a {Sample} of {Novice} {Riders} in a {Canadian} {City}},
	copyright = {© 2018 Canadian Association of Geographers / L'Association canadienne des géographes},
	doi = {10.1111/cag.12456},
	abstract = {Key Messages E-bikes are an under-studied emerging technology with potential to facilitate sustainability agendas in urban transportation (e.g. active, multi-modal, low-carbon mobility). Little to no research has been conducted on e-bikes in Canadian urban contexts where sprawl and car dependency are predominant. E-bikes may be a viable substitute for cars for shorter commutes, yet weight, winter weather, and battery life remain barriers.},
	journal = {The Canadian Geographer / Le Géographe canadien},
	author = {Edge, S. and Dean, J. and Cuomo, M. and Keshav, S.},
	year = {2018},
	pages = {384--397},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\471\\Edge et al. - 2018 - Exploring e-bikes as a mode of sustainable transpo.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\470\\cag.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\VPTWCBIV\\Edge et al. - 2018 - Exploring e-bikes as a mode of sustainable transpo.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\YW4UUB6M\\cag.html:text/html},
}

@book{government_of_canada_daily_2017,
	title = {The {Daily} — {Journey} to {Work}: {Key} {Results} from the 2016 {Census}},
	url = {http://www150.statcan.gc.ca/n1/daily-quotidien/171129/dq171129c-eng.htm},
	abstract = {More Canadians commuted to work in 2016 and a greater proportion took public transit than ever before. Since 1996, the number of commuters has risen by 3.7 million or 30.3\% to 15.9 million in 2016. However, how they get to work is changing.},
	author = {Government of Canada, Statistics Canada},
	month = nov,
	year = {2017},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\473\\dq171129c-eng.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\NVW468MK\\dq171129c-eng.html:text/html},
}

@article{cabral_bicycle_2018,
	title = {Bicycle {Ridership} and {Intention} in a {Northern}, {Low}-{Cycling} {City}},
	doi = {10.1016/j.tbs.2018.08.005},
	abstract = {Cycling as a mode of transportation (i.e. utility cycling) has been given heavy attention and investment in North America over the last decade. It is perceived as an environmentally friendly way to travel, leading to benefits for health and traffic alleviation. This study examines the determinants of utility cycling behaviour and intent, and more broadly, active transportation (i.e. cycling and walking) behaviour in Edmonton, Canada – the northernmost North American city with a metropolitan population over one million. With harsh winter weather and low cycling rates, the city presents a unique case study for cycling behavior. In this research, we analyzed 646 responses to a bike ridership survey conducted in 2014 by the City of Edmonton. Borrowing concepts from behaviour theory, public health and transportation engineering we seek to quantify the effects of infrastructure density, traffic attitude, perceived control over time and distance, and traffic stress tolerance perception on cycling for utility purposes, the intention to cycle more frequently, and the use of an active mode of transportation, specifically for a northern and low-cycling city. Three empirical models were developed to describe cycling behaviour using binary logistic regression. Most variables were significant and in line with other study findings in the current literature. Results point at the importance of perceived safety in deciding or intending to cycle, as well as perceived time and distance of travel. Broad policy implications and suggestions for future research are discussed.},
	journal = {Travel Behaviour and Society},
	author = {Cabral, Laura and Kim, Amy M. and Parkins, John R.},
	month = oct,
	year = {2018},
	pages = {165--173},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\476\\Cabral et al. - 2018 - Bicycle ridership and intention in a northern, low.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\475\\S2214367X18300565.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\RNWW3QJR\\Cabral et al. - 2018 - Bicycle ridership and intention in a northern, low.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\VZ6M3ZLG\\S2214367X18300565.html:text/html},
}

@article{schneider_theory_2013,
	title = {Theory of {Routine} {Mode} {Choice} {Decisions}: {An} {Operational} {Framework} to {Increase} {Sustainable} {Transportation}},
	doi = {10.1016/j.tranpol.2012.10.007},
	abstract = {A growing number of communities in the United States are seeking to improve the sustainability of their transportation systems by shifting routine automobile travel to walking and bicycling. In order to identify strategies that may be most effective at increasing pedestrian and bicycle transportation in a specific local context, practitioners need a greater understanding of the underlying thought process that people use to select travel modes. Previous research from the travel behavior and psychology fields provides the foundation for a five-step, operational Theory of Routine Mode Choice Decisions. Walking and bicycling could be promoted through each of the five steps: awareness and availability (e.g., offer individual marketing programs), basic safety and security (e.g., make pedestrian and bicycle facility improvements and increase education and enforcement efforts), convenience and cost (e.g., institute higher-density, mixed land uses, and limited, more expensive automobile parking), enjoyment (e.g., plant street trees and increase awareness of non-motorized transportation benefits), and habit (e.g., target information about sustainable transportation options to people making key life changes). The components of the theory are supported by in-depth interview responses from the San Francisco Bay Area.},
	journal = {Transport Policy},
	author = {Schneider, R. J.},
	month = jan,
	year = {2013},
	pages = {128--137},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\479\\Schneider - 2013 - Theory of routine mode choice decisions An operat.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\478\\S0967070X12001643.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\EJWSKBDX\\Schneider - 2013 - Theory of routine mode choice decisions An operat.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\TY9KA7RG\\S0967070X12001643.html:text/html},
}

@book{smith_ill_2011,
	title = {'{I}'ll {Just} {Take} the {Car}': {Improving} {Bicycle} {Transportation} to {Encourage} {Its} {Use} on {Short} {Trips}},
	isbn = {978-0-478-37125-3 978-0-478-37126-0},
	url = {http://www.nzta.govt.nz/resources/research/reports/426/docs/426.pdf},
	abstract = {This research, from July 2008 to June 2010, applied the affective design methodology to the goal of increasing practical cycling in New Zealand. A literature review revealed that overseas best practice is for integrated local cycling policies. Theories of diffusion of innovations and contemplation of change were highlighted and used to inform the project. A review of the New Zealand cycling market showed limited choice of and access to practical cycling tools. A survey of 234 New Zealand cyclists and non-cyclists demonstrated differences between the groups in perception of bicycles and cyclists, with more agreement for unfamiliar practical cyclists and bicycles. Practical workshops explored the effect of direct cycling experience on perceptions. A "practical cycling system design model" was proposed, along with recommendations for its implementation.},
	publisher = {NZ Transport Agency},
	author = {Smith, Paul and Wilson, Mike and Armstrong, Tim and {NZ Transport Agency}},
	year = {2011},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\481\\Smith et al. - 2011 - 'I'll just take the car' improving bicycle transp.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\4I8JFEYS\\Smith et al. - 2011 - 'I'll just take the car' improving bicycle transp.pdf:application/pdf;Full Text:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\M2QQ7JC3\\Smith et al. - 2011 - 'I'll Just Take the Car' Improving Bicycle Transp.pdf:application/pdf},
}

@book{pjcci_pietons_2019,
	title = {Piétons et cyclistes \${\textbackslash}star\$ {PJCCI}},
	url = {https://jacquescartierchamplain.ca/circulation-travaux/pietons-et-cyclistes/},
	abstract = {Sur cette page, les piétons et les cyclistes trouveront diverses informations sur les pistes qu'ils peuvent explorer, les points de vue à découvrir ainsi que la saison d'exploitation. Cette année en grande nouveauté, un nouvel affichage vous permet de savoir en un coup d'øeil si la piste est ouverte ou fermée. Projet de simulation d'exploitation hivernale {\textbackslash}ldots},
	author = {{PJCCI}},
	year = {2019},
	file = {Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\482\\Piste_multifonctionnelle_du_pont_Jacques-Cartier_simulation_exploitation_hivernale_2019-11-07.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Documents\\Zotero-My Library\\files\\484\\pietons-et-cyclistes.html:text/html;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\DJFL62BN\\Piste_multifonctionnelle_du_pont_Jacques-Cartier_simulation_exploitation_hivernale_2019-11-07.pdf:application/pdf;Attachment:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\VDPVQQE2\\pietons-et-cyclistes.html:text/html},
}

@book{dustin_realtime_2019,
	title = {Realtime {Semantic} {Segmentation} on {Jetson} {Nano} in {Python} and {C}++},
	url = {https://www.linkedin.com/pulse/realtime-semantic-segmentation-jetson-nano-python-c-dustin-franklin},
	abstract = {Recently I updated the Hello AI World project on GitHub with new semantic segmentation models based on FCN-ResNet18 that run in realtime on Jetson Nano, in addition to Python bindings and examples for segmenting images and live camera video. Like with the other networks in the project, they use NVID},
	author = {Dustin, F.},
	month = oct,
	year = {2019},
}

@incollection{zheng_real-time_2020,
	title = {Real-{Time} {Semantic} {Segmentation} {Network} for {Edge} {Deployment}},
	isbn = {978-981-329-697-8 978-981-329-698-5},
	url = {http://link.springer.com/10.1007/978-981-32-9698-5_28},
	abstract = {In this paper, we focus on the embedded deployment of real-time semantic segmentation network. Semantic segmentation based on convolutional neural networks has achieved impressive success in recent years, thus raising interests of researchers in many related application fields. The practical applications such as autonomous driving raise challenge to the lightweight of networks. Many previous achievements lighten the network by reducing layers, channels and applying group convolution, depthwise convolution to get realtime performance. At a cost, the learning ability of the network decreases. To break out of the dilemma, we propose MFANet with efficient multi-fiber unit and attention module, which obtains well balance between speed and performance. Testing on single NVIDIA 1080Ti GPU, the network achieves 65.71\% MIoU with only 3.472 GFLOPs and speed of 135 FPS on the Cityscapes dataset as the input size 512 Â 1024. Further on, after some adjustments, we deploy the network to NVIDIA jetson nano embedded system, it achieves 55.33\% MIoU and speed of 12 FPS, to further accelerate the model, we converted the trained model into tensorrt model of type int8, it achieves 54. 47\% MIoU and speed of 47 FPS, which is capable of industrial deployment.},
	booktitle = {Proceedings of 2019 {Chinese} {Intelligent} {Systems} {Conference}},
	publisher = {Springer Singapore},
	author = {Zheng, J. and Li, J. and Liu, Y. and Zhang, W.},
	editor = {Jia, Y. and Du, J. and Zhang, W.},
	year = {2020},
	doi = {10.1007/978-981-32-9698-5_28},
	pages = {243--249},
}

@misc{pjcci_rapport_2018,
	title = {Rapport post-mortem sur le projet pilote d'entretien hivernal de la piste multifonctionnelle du pont {Jacques}-{Cartier}},
	url = {https://jacquescartierchamplain.ca/wp-content/uploads/2018/10/RPP_piste_PJC_2018-10-10-1.pdf},
	author = {PJCCI},
	month = oct,
	year = {2018},
	file = {RPP_piste_PJC_2018-10-10-1.pdf:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\BUE6K587\\RPP_piste_PJC_2018-10-10-1.pdf:application/pdf},
}

@misc{pjcci_fiche_2018,
	title = {Fiche de la piste multifonctionelle du pont {Jacques}-{Cartier}},
	url = {https://jacquescartierchamplain.ca/wp-content/uploads/2018/10/IMG_Fiche_piste-multi_pont_JC_FR_vfinale_web__2018-10-10.pdf},
	author = {PJCCI},
	month = oct,
	year = {2018},
	file = {IMG_Fiche_piste-multi_pont_JC_FR_vfinale_web__2018-10-10.pdf:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\REB7UMJ6\\IMG_Fiche_piste-multi_pont_JC_FR_vfinale_web__2018-10-10.pdf:application/pdf},
}

@misc{association_des_pietons_et_cyclistes_du_pont_jacques-cartier_pontjacques-cartier365com_2020,
	title = {{PontJacques}-{Cartier365}.com},
	url = {http://pontjacquescartier365.com},
	author = {Association des piétons et cyclistes du pont Jacques-Cartier},
	year = {2020},
}

@misc{association_des_pietons_et_cyclistes_pont_jacques-cartier_flickr_2020,
	title = {Flickr {Association} des piétons et cyclistes pont {Jacques}-{Cartier}},
	url = {https://www.flickr.com/photos/pontjacquescartier},
	author = {Association des piétons et cyclistes pont Jacques-Cartier},
	year = {2020},
}

@book{wang_real-time_2017,
	address = {Hoboken, NJ, USA},
	series = {Quantitative software engineering series},
	title = {Real-time embedded systems},
	isbn = {978-1-119-42070-5 978-1-119-42068-2},
	language = {en},
	publisher = {Wiley},
	author = {Wang, Jiacun},
	year = {2017},
	keywords = {Embedded computer systems, Real-time data processing, TECHNOLOGY \& ENGINEERING / Electronics / Microelectronics},
	file = {Wang - 2017 - Real-time embedded systems.pdf:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\TX9JAPK6\\Wang - 2017 - Real-time embedded systems.pdf:application/pdf},
}

@book{chollet_deep_2018,
	address = {Shelter Island, New York},
	title = {Deep learning with {Python}},
	isbn = {978-1-61729-443-3},
	language = {en},
	publisher = {Manning Publications Co},
	author = {Chollet, François},
	year = {2018},
	note = {OCLC: ocn982650571},
	keywords = {Machine learning, Neural networks (Computer science), Python (Computer program language)},
	file = {Chollet - 2018 - Deep learning with Python.pdf:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\I5MWEI3Z\\Chollet - 2018 - Deep learning with Python.pdf:application/pdf},
}

@article{lecun_deep_nodate,
	title = {Deep {Learning}: {Past}, {Present} and {Future}},
	language = {en},
	author = {LeCun, Yann},
	pages = {134},
	file = {LeCun - Deep Learning Past, Present and Future.pdf:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\SDR85JX7\\LeCun - Deep Learning Past, Present and Future.pdf:application/pdf},
}

@article{wu_recent_2019,
	title = {Recent {Advances} in {Deep} {Learning} for {Object} {Detection}},
	url = {http://arxiv.org/abs/1908.03673},
	abstract = {Object detection is a fundamental visual recognition problem in computer vision and has been widely studied in the past decades. Visual object detection aims to ﬁnd objects of certain target classes with precise localization in a given image and assign each object instance a corresponding class label. Due to the tremendous successes of deep learning based image classiﬁcation, object detection techniques using deep learning have been actively studied in recent years. In this paper, we give a comprehensive survey of recent advances in visual object detection with deep learning. By reviewing a large body of recent related work in literature, we systematically analyze the existing object detection frameworks and organize the survey into three major parts: (i) detection components, (ii) learning strategies, and (iii) applications \& benchmarks. In the survey, we cover a variety of factors affecting the detection performance in detail, such as detector architectures, feature learning, proposal generation, sampling strategies, etc. Finally, we discuss several future directions to facilitate and spur future research for visual object detection with deep learning.},
	language = {en},
	urldate = {2020-08-09},
	journal = {arXiv:1908.03673 [cs]},
	author = {Wu, Xiongwei and Sahoo, Doyen and Hoi, Steven C. H.},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.03673},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
	file = {Wu et al. - 2019 - Recent Advances in Deep Learning for Object Detect.pdf:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\22KE8XRP\\Wu et al. - 2019 - Recent Advances in Deep Learning for Object Detect.pdf:application/pdf},
}

@article{minaee_image_2020,
	title = {Image {Segmentation} {Using} {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Image {Segmentation} {Using} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2001.05566},
	abstract = {Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others. Various algorithms for image segmentation have been developed in the literature. Recently, due to the success of deep learning models in a wide range of vision applications, there has been a substantial amount of works aimed at developing image segmentation approaches using deep learning models. In this survey, we provide a comprehensive review of the literature at the time of this writing, covering a broad spectrum of pioneering works for semantic and instance-level segmentation, including fully convolutional pixel-labeling networks, encoder-decoder architectures, multi-scale and pyramid based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the similarity, strengths and challenges of these deep learning models, examine the most widely used datasets, report performances, and discuss promising future research directions in this area.},
	language = {en},
	urldate = {2020-08-09},
	journal = {arXiv:2001.05566 [cs]},
	author = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
	month = apr,
	year = {2020},
	note = {arXiv: 2001.05566},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Minaee et al. - 2020 - Image Segmentation Using Deep Learning A Survey.pdf:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\CYAUGX3K\\Minaee et al. - 2020 - Image Segmentation Using Deep Learning A Survey.pdf:application/pdf},
}

@article{cornioley_integration_2018,
	title = {Intégration d’un module d’apprentissage profond dans l’architecture logicielle d’un {SIG} {Web}},
	language = {fr},
	author = {Cornioley, Paul},
	month = may,
	year = {2018},
	pages = {90},
	file = {Cornioley - Intégration d’un module d’apprentissage profond da.pdf:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\6ZI7ZF73\\Cornioley - Intégration d’un module d’apprentissage profond da.pdf:application/pdf},
}

@incollection{forsyth_segmentation_2008,
	address = {Berlin, Heidelberg},
	title = {Segmentation and {Recognition} {Using} {Structure} from {Motion} {Point} {Clouds}},
	volume = {5302},
	isbn = {978-3-540-88681-5 978-3-540-88682-2},
	url = {http://link.springer.com/10.1007/978-3-540-88682-2_5},
	abstract = {We propose an algorithm for semantic segmentation based on 3D point clouds derived from ego-motion. We motivate ﬁve simple cues designed to model speciﬁc patterns of motion and 3D world structure that vary with object category. We introduce features that project the 3D cues back to the 2D image plane while modeling spatial layout and context. A randomized decision forest combines many such features to achieve a coherent 2D segmentation and recognize the object categories present. Our main contribution is to show how semantic segmentation is possible based solely on motion-derived 3D world structure. Our method works well on sparse, noisy point clouds, and unlike existing approaches, does not need appearance-based descriptors.},
	language = {en},
	urldate = {2021-05-08},
	booktitle = {Computer {Vision} – {ECCV} 2008},
	publisher = {Springer Berlin Heidelberg},
	author = {Brostow, Gabriel J. and Shotton, Jamie and Fauqueur, Julien and Cipolla, Roberto},
	editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
	year = {2008},
	doi = {10.1007/978-3-540-88682-2_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {44--57},
	file = {Brostow et al. - 2008 - Segmentation and Recognition Using Structure from .pdf:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\WLPLH3KW\\Brostow et al. - 2008 - Segmentation and Recognition Using Structure from .pdf:application/pdf},
}

@misc{kilby_nobel_2000,
	title = {The {Nobel} {Prize} in {Physics} 2000},
	url = {https://www.nobelprize.org/prizes/physics/2000/kilby/lecture/},
	abstract = {The Nobel Prize in Physics 2000 was awarded "for basic work on information and communication technology" with one half jointly to Zhores I. Alferov and Herbert Kroemer "for developing semiconductor heterostructures used in high-speed- and opto-electronics" and the other half to Jack S. Kilby "for his part in the invention of the integrated circuit".},
	language = {en-US},
	urldate = {2021-10-09},
	journal = {NobelPrize.org},
	author = {Kilby, Jack S.},
	year = {2000},
	file = {kilby-lecture.pdf:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\4CV3Q6TF\\kilby-lecture.pdf:application/pdf;Snapshot:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\EVTBBEXB\\lecture.html:text/html},
}

@misc{nvidia_nvidia_2020,
	title = {{NVIDIA} {Jetson} {Linux} {Developer} {Guide} : {Jetson} {Module} {Support} {\textbar} {NVIDIA} {Docs}},
	url = {https://docs.nvidia.com/jetson/archives/l4t-archived/l4t-3242/index.html#page/Tegra%20Linux%20Driver%20Package%20Development%20Guide/jetson_module_support.html},
	urldate = {2021-10-09},
	author = {NVIDIA},
	year = {2020},
	file = {NVIDIA Jetson Linux Developer Guide \: Jetson Module Support | NVIDIA Docs:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\RTCZPBWV\\index.html:text/html},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2021-11-20},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\CNHY393P\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\AXF88AHQ\\1512.html:text/html},
}

@article{rodriguez-conde_-device_2021,
	title = {On-device object detection for more efficient and privacy-compliant visual perception in context-aware systems},
	volume = {11},
	issn = {2076-3417},
	doi = {10.3390/app11199173},
	abstract = {Ambient Intelligence (AmI) encompasses technological infrastructures capable of sensing data from environments and extracting high-level knowledge to detect or recognize users’ features and actions, as well as entities or events in their surroundings. Visual perception, particularly object detection, has become one of the most relevant enabling factors for this context-aware user-centered intelligence, being the cornerstone of relevant but complex tasks, such as object tracking or human action recognition. In this context, convolutional neural networks have proven to achieve state-of-the-art accuracy levels. However, they typically result in large and highly complex models that typically demand computation offloading onto remote cloud platforms. Such an approach has security-and latency-related limitations and may not be appropriate for some AmI use cases where the system response time must be as short as possible, and data privacy must be guaranteed. In the last few years, the on-device paradigm has emerged in response to those limitations, yielding more compact and efficient neural networks able to address inference directly on client machines, thus providing users with a smoother and better-tailored experience, with no need of sharing their data with an outsourced service. Framed in that novel paradigm, this work presents a review of the recent advances made along those lines in object detection, providing a comprehensive study of the most relevant lightweight CNN-based detection frameworks, discussing the most paradigmatic AmI domains where such an approach has been successfully applied, the different challenges arisen, the key strategies and techniques adopted to create visual solutions for image-based object classification and localization, as well as the most relevant factors to bear in mind when assessing or comparing those techniques, such as the evaluation metrics or the hardware setups used. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	language = {English},
	number = {19},
	journal = {Applied Sciences (Switzerland)},
	author = {Rodriguez-Conde, I. and Campos, C. and Fdez-Riverola, F.},
	year = {2021},
	keywords = {Ambient intelligence, Convolutional neural networks, Deep learning, Object detection, On-device},
	file = {Full Text:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\H54ICK27\\Rodriguez-Conde et al. - 2021 - On-device object detection for more efficient and .pdf:application/pdf;Snapshot:C\:\\Users\\vincent.le_falher\\Zotero\\storage\\HC6Q8N8M\\display.html:text/html},
}
