<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Deep Learning 101 - Part 1: History and Background</title>
  <meta name="description" content="The first in a multipart series on getting started with deep learning. In this part we will cover the history of deep learning to figure out how we got here,...">

  <link rel="stylesheet" href="foundation.css">
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="fontello.css">
  <link rel="stylesheet" href="font-awesome.css">

  <script src="libs.html" type="text/javascript"></script>
  <script>
    // terrificjs bootstrap
    (function($) {
        $(document).ready(function() {
            var $page = $('body');
            var config = {
              dependencyPath: {
                plugin: 'javascripts/'
              }
            }
            var application = new Tc.Application($page, config);
            application.registerModules();
            application.start();
        });
    })(Tc.$);
  </script>

  <script>
  function resizeIframe(obj) {
    obj.style.height = obj.contentWindow.document.body.scrollHeight + 'px';
  }
</script>

  <link href="css" media="screen" rel="stylesheet" type="text/css">
  <script src="masonry.js" type="text/javascript"></script>
  <script src="imagesloaded.js" type="text/javascript"></script>
  <script src="slick.js" type="text/javascript"></script>
  <script src="particles.js" type="text/javascript"></script>


    <script src="d3.js" type="text/javascript"></script>
    <script src="ldavis.js" type="text/javascript"></script>
    <link rel="stylesheet" type="text/css" href="lda.css">

  <link rel="canonical" href="https://beamandrew.github.io/deeplearning/2017/02/23/beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html">
  <link rel="alternate" type="application/rss+xml" title="beamlab" href="https://beamandrew.github.io/deeplearning/2017/02/23/beamandrew.github.io/feed.xml">
<link rel="icon" type="image/png" href="https://beamandrew.github.io/images/tab-logo.png"><script type="text/javascript" async="" src="embed.js"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><link rel="prefetch" as="style" href="https://c.disquscdn.com/next/embed/styles/lounge.953a2bd009935f47a8e815c3ee2bfc5a.css"><link rel="prefetch" as="script" href="https://c.disquscdn.com/next/embed/common.bundle.9ae27258a9490b17fbb3b9cdf530aff0.js"><link rel="prefetch" as="script" href="https://c.disquscdn.com/next/embed/lounge.bundle.0adc4cfceff8c3ab4259d467d6ea3419.js"><link rel="prefetch" as="script" href="https://disqus.com/next/config.js"><script charset="utf-8" src="button.js"></script></head>

  

  <body><div id="MathJax_Message" style="display: none;"></div>

    <a href="" id="top"></a>
<div class="contain-to-grid shadowless sticky">
  <nav class="top-bar" data-options="sticky_on: large" data-topbar="">
    <ul class="title-area">
      <li class="name">
        <h1>
          <a href="https://beamandrew.github.io/deeplearning/2017/02/23/beamandrew.github.io/index.html">
            <img alt="" src="logo.html">
          </a>
        </h1>
      </li>
      <li class="toggle-topbar menu-icon">
        <a href="#">Menu</a>
      </li>
    </ul>
    <section class="top-bar-section">
      <ul class="right">
        
          

          
            <li class="">
              <a href="https://beamandrew.github.io/deeplearning/2017/02/23/beamandrew.github.io/index.html">Home</a>
            </li>
          
        
          

          
            <li class="has-dropdown ">
              <a href="https://beamandrew.github.io/deeplearning/2017/02/23/beamandrew.github.io/research.html">Research</a>
              <ul class="dropdown">
                
                  <li>
                    <a href="https://beamandrew.github.io/deeplearning/2017/02/23/beamandrew.github.io/overview.html">Overview</a>
                  </li>
                
                  <li>
                    <a href="https://beamandrew.github.io/deeplearning/2017/02/23/beamandrew.github.io/research.html">Publications</a>
                  </li>
                
                  <li>
                    <a href="https://beamandrew.github.io/deeplearning/2017/02/23/beamandrew.github.io/press.html">Press</a>
                  </li>
                
              </ul>
            </li>
          
        
          

          
            <li class="">
              <a href="https://beamandrew.github.io/deeplearning/2017/02/23/beamandrew.github.io/team.html">Team</a>
            </li>
          
        
          

          
            <li class="">
              <a href="https://beamandrew.github.io/deeplearning/2017/02/23/beamandrew.github.io/positions.html">Positions</a>
            </li>
          
        
          

          
            <li class="">
              <a href="https://beamandrew.github.io/deeplearning/2017/02/23/beamandrew.github.io/blog/">Blog</a>
            </li>
          
        
      </ul>
    </section>
  </nav>
</div>

    <div id="main" role="main">
      <div class="full">
  <div class="row">
    <div class="large-9 columns">
      <div class="mod modBlogPost big no_bg">
  <div class="images">
    
      <div class="image"><img alt="" src="nn_timeline.html"></div>
    
  </div>
  <div class="content">
    <p class="info">
      <span>February 23, 2017</span>
      /
      <span>
        by
        <a href=""></a>
      </span>
      /
      <span>
        In
        
          <a href="#">deeplearning</a>
          
        
      </span>
    </p>
    <h3><a href="https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html">Deep Learning 101 - Part 1: History and Background</a></h3>
    <script src="MathJax.js" id=""></script>

<h4 id="tldr-the-first-in-a-multipart-series-on-getting-started-with-deep-learning-in-this-part-we-will-cover-the-history-of-deep-learning-to-figure-out-how-we-got-here-plus-some-tips-and-tricks-to-stay-current"><strong>tl;dr:</strong>
 The first in a multipart series on getting started with deep learning. 
In this part we will cover the history of deep learning to figure out 
how we got here, plus some tips and tricks to stay current.</h4>

<p><em>The Deep Learning 101 series is a companion piece to a talk given
 as part of the Department of Biomedical Informatics @ Harvard Medical 
School ‘Open Insights’ series. Slides for the talk are available <a href="https://slides.com/beamandrew/deep-learning-101/">here</a> and a recording is also available on <a href="https://youtu.be/xuIKzt5G21c">youtube</a></em></p>

<h3 id="other-posts-in-this-series">Other Posts in this Series</h3>

<ul>
  <li><a href="http://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part2.html"><strong>Part 2: Multilayer Perceptrons</strong></a></li>
</ul>

<p>Each post in this series is a collection of explanations, references 
and pointers meant to help someone new to the field quickly bootstrap 
their knowledge of key events, people, and terms in deep learning. In 
the same way that neural nets use a <a href="http://www.cs.toronto.edu/%7Ebonner/courses/2014s/csc321/lectures/lec5.pdf">distributed representation</a>
 to process data, reference materials for deep learning are scattered 
across the far flung corners of the internet and embedded in the <a href="http://www.infotoday.eu/Articles/Editorial/Featured-Articles/The-dark-matter-of-the-internet-100102.aspx">dark ether</a> of social media. The hope is that coalescing at least <em>some</em>
 of these materials into a central location will make it easier for new 
comers to start their own walk over this knowledge graph. This 
collection is intentionally peppered with trivia and articles from the 
popular press that are relevant to deep learning to keep things 
interesting and to provide context.</p>

<p>These posts are also inspired by the Matt Might <a href="http://matt.might.net/articles/how-to-blog-as-an-academic/">Mantra</a> of blogging:</p>

<blockquote>
  <p>The secret to low-cost academic blogging is to make blogging a natural byproduct of all the things that academics already do.</p>
</blockquote>

<p>If Matt Might gives a suggestion, it’s probably a good idea to follow it.</p>

<p>I hope to keep this updated and fresh as new research is produced. 
Hopefully this can remain a point of reference in the future. So like 
Kanye’s <a href="http://consequenceofsound.net/2016/03/kanye-west-updates-12-tracks-on-the-life-of-pablo-album-available-everywhere-on-friday/">Life of Pablo</a>,
 the posts in this series will continue to change and evolve as new 
stuff happens. If you see something missing that you think should be 
added, leave a comment below or shoot me a message on <a href="https://twitter.com/AndrewLBeam">twitter</a>.</p>

<h3 id="introduction">Introduction</h3>
<p>Deep learning, over the past 5 years or so, has gone from a somewhat 
niche field comprised of a cloistered group of researchers to being so 
mainstream that even <a href="https://en.wikipedia.org/wiki/Kristen_Stewart">that girl</a> from <a href="https://en.wikipedia.org/wiki/The_Twilight_Saga_%28film_series%29">Twilight</a> has published a deep learning <a href="https://arxiv.org/abs/1701.04928">paper</a>.
 The swift rise and apparent dominance of deep learning over traditional
 machine learning methods on a variety of tasks has been astonishing to 
witness, and at times difficult to explain. There is a fascinating 
history that goes back to the 1940s full of ups and downs, twists and 
turns, friends and rivals, and successes and failures. In a story arc 
worthy of a 90s <a href="https://en.wikipedia.org/wiki/She%27s_All_That">movie</a>, an idea that was once sort of an <a href="https://plus.google.com/+YannLeCunPhD/posts/gurGyczzsJ7">ugly duckling</a> has blossomed to become the <a href="https://www.wired.com/2014/01/geoffrey-hinton-deep-learning/">belle of the ball</a>.</p>

<p>Consequently, interest in deep learning has sky-rocketed, with <a href="https://www.nytimes.com/2016/12/05/technology/uber-bets-on-artificial-intelligence-with-acquisition-and-new-lab.html?_r=0">constant</a> <a href="http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-intelligence.html">coverage</a> in the <a href="https://www.bloomberg.com/news/articles/2016-12-06/apple-to-start-publishing-ai-research-to-hasten-deep-learning">popular</a> media. Deep learning research now routinely appears in top journals like <a href="http://science.sciencemag.org/content/early/2014/12/17/science.1254806.full">Science</a>, <a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">Nature</a>, <a href="http://www.nature.com/nmeth/journal/vaop/ncurrent/full/nmeth.4182.html">Nature Methods</a> and <a href="http://jamanetwork.com/journals/jama/article-abstract/2588763">JAMA</a> just to name a few. Deep learning has conquered <a href="http://www.nature.com/news/google-reveals-secret-test-of-ai-bot-to-beat-top-go-players-1.21253">Go</a>, learned to <a href="https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/">drive a car</a>, diagnosed <a href="http://www.nature.com/nature/journal/v542/n7639/full/nature21056.html">skin cancer</a> and <a href="http://www.nature.com/nature/journal/v542/n7641/full/nature21369.html">autism</a>, became a <a href="https://arxiv.org/abs/1508.06576">master</a> art <a href="http://genekogan.com/works/style-transfer/">forger</a>, and can even hallucinate photorealistic <a href="https://github.com/Newmu/dcgan_code">pictures</a>.</p>

<p>A good surrogate for interest in deep learning is attendance at the 
Annual Conference on Neural Information Processing Systems (NIPS). NIPS 
is the main conference for deep learning research and has historically 
been where a lot of the new methodological research get published. 
Interest in the conference has surged in the last 5 years:</p>

<p><img src="nips.html" alt="NIPS" class="center-image" width="512px" height="432px"></p>

<p>The chart only goes to 2015 and this year’s <a href="http://beamandrew.github.io/deeplearning/2016/12/12/nips-2016.html">NIPS</a>
 had over 6,000 attendees, so it doesn’t look like interest is close to 
leveling off anytime soon. Confirmation that we were entering a period 
of serious hype occurred during NIPS 2013 when Mark Zuckerberg made a <a href="http://blog.mikiobraun.de/2013/12/mark-zuckerberg-nips.html">surprise</a> visit to recruit deep learning talent, and ended up <a href="https://www.facebook.com/yann.lecun/posts/10151728212367143">convincing</a> <a href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann Lecun</a> to be the director of the Facbook <a href="https://research.fb.com/category/facebook-ai-research-fair/">AI lab</a>.</p>

<p>What does all this hype mean? Why is this happening now? At a 
high-level, what we’ve witnessed is the synergistic combination of 
insights from optimization, traditional machine learning, software 
engineering, integrated circuit design, and serious sweat equity from a 
dedicated group of researchers, postdocs, and <a href="https://sciencedryad.wordpress.com/2014/01/25/grad-student-descent/">grad students</a>.
 Though the main ideas behind deep learning have been in place for 
decades, it wasn’t until data sets became large enough and computers got
 fast enough that their true power could be revealed. With that said, 
it’s worth walking through the history of neural nets and deep learning 
to see how we got here.</p>

<h3 id="milestones-in-the-development-of-neural-networks">Milestones in the Development of Neural Networks</h3>
<p><img src="nn_timeline.html" alt="Timeline" class="center-image" width="1067px" height="501px">
<strong>Note: I am not the creator of the above image. I have been 
unable to locate the original source. If this is your image please let 
me know so I can give you proper attribution.</strong></p>

<h3 id="in-the-beginning-1940s">In The Beginning (1940s)…</h3>
<p>The idea of creating a ‘thinking’ machine is at least as old as modern computing, if not <a href="https://en.wikipedia.org/wiki/Golem">even</a> <a href="https://motherboard.vice.com/en_us/article/forget-turing-the-lovelace-test-has-a-better-shot-at-spotting-ai">older</a>. Alan Turing in his seminal paper <a href="https://www.csee.umbc.edu/courses/471/papers/turing.pdf">‘Computing Machinery and Intelligence’</a> laid out several criteria to asses whether a machine could be said be intelligent, which has since become known as the <a href="https://en.wikipedia.org/wiki/Turing_test">‘Turing test’</a>.
 For some great explorations on variants of the Turing test, check-out 
Brain Christian’s book detailing his adventures with the <a href="https://en.wikipedia.org/wiki/Loebner_Prize">Loebner Prize</a> entitled <a href="http://www.nytimes.com/2011/03/20/books/review/book-review-the-most-human-human-by-brian-christian.html"><em>The Most Human Human</em></a> or check the amazing, dramatized version in <a href="https://en.wikipedia.org/wiki/Ex_Machina_%28film%29">Ex-machina</a>.</p>

<p>Early work in machine learning was largely informed by the current 
working theories of the brain. The first guys on the scene were Walter 
Pitts and Warren McCulloch. They had developed a technique known as 
“thresholded logic unit” and was designed to mimic the way a neuron was 
thought to work (which will be a recurring theme). There is a truly 
great story on the partnership of McCulloch and Pitts, available <a href="http://nautil.us/issue/21/information/the-man-who-tried-to-redeem-the-world-with-logic">here</a> that I highly recommend. Here’s a quote to get you started.</p>

<blockquote>
  <p>McCulloch was a confident, gray-eyed, wild-bearded, chain-smoking 
philosopher-poet who lived on whiskey and ice cream and never went to 
bed before 4 a.m.</p>
</blockquote>

<p>Sounds like my kind of person. Picking the thread back up again, it isn’t until <a href="https://en.wikipedia.org/wiki/Frank_Rosenblatt">Frank Rosenblatt’s</a> <a href="https://en.wikipedia.org/wiki/Perceptron">“perceptron”</a>
 that we see the first real precursor to modern neural networks. For its
 day, this thing was pretty impressive. It came with a learning 
procedure that would provably converge to the correct solution and could
 recognize letters and numbers. Rosenblatt was so confident that the 
perceptron would lead to true AI, that in 1959 he remarked:</p>

<blockquote>
  <p>[The perceptron is] the embryo of an electronic computer that [the 
Navy] expects will be able to walk, talk, see, write, reproduce itself 
and be conscious of its existence.</p>
</blockquote>

<p><a href="http://www.dictionary.com/browse/dramatic-irony">Queue</a> the ominous music.</p>

<h3 id="the-first-ai-winter-1969">The First AI Winter (1969)</h3>
<p><img src="ai_winter.html" alt="AI Winter" class="center-image" width="300px" height="300px">
Rosenblatt’s perceptron began to garner quite a bit of attention, and one person in particular began to take notice. <a href="https://en.wikipedia.org/wiki/Marvin_Minsky">Marvin Minsky</a>, who is often thought of as one of the <a href="https://www.technologyreview.com/s/546116/what-marvin-minsky-still-means-for-ai/">father’s</a> of AI, began to sense that something was off with Rosenblatt’s perceptron. Minsky is quoted <a href="http://www.newyorker.com/magazine/1981/12/14/a-i">here</a> saying:</p>

<blockquote>
  <p>However, I started to worry about what such a machine could not do.
 For example, it could tell ‘E’s from ‘F’s, and ‘5’s from ‘6’s—things 
like that. But when there were disturbing stimuli near these figures 
that weren’t correlated with them the recognition was destroyed.</p>
</blockquote>

<p>Along with the double-PhD wielding <a href="https://en.wikipedia.org/wiki/Seymour_Papert">Seymor Papert</a>, Minksy wrote a book entitled <a href="https://en.wikipedia.org/wiki/Perceptron"><em>Perceptrons</em></a>
 that effectively killed the perceptron, ending embryonic idea of a 
neural net. They showed that the perceptron was incapable of learning 
the simple exclusive-or (XOR) function. Worse, they <em>proved</em> that
 it was theoretically impossible for it to learn such a function, no 
matter how long you let it train. Now this isn’t surprising to us, as 
the model implied by the perceptron is a linear one and the XOR function
 is nonlinear, but at the time this was enough to kill all research on 
neural nets and usher in the first AI winter.</p>

<p>It’s hard to be mad at Minsky though, and after reading this New Yorker <a href="http://www.newyorker.com/magazine/1981/12/14/a-i">piece</a>,
 he sounds like someone who is as close to the platonic ideal of a 
scientist and academic as a human can be. Here is a quote from the 
article the entice you to go read it:</p>

<blockquote>
  <p>When he was a student, he has said, there appeared to him to be 
only three interesting problems in the world—or in the world of science,
 at least. “Genetics seemed to be pretty interesting, because nobody 
knew yet how it worked,” he said. “But I wasn’t sure that it was 
profound. The problems of physics seemed profound and solvable. It might
 have been nice to do physics. But the problem of intelligence seemed 
hopelessly profound. I can’t remember considering anything else worth 
doing.”</p>
</blockquote>

<h3 id="the-backpropagandists-emerge-1986">The Backpropagandists Emerge (1986)</h3>
<p>As the Minksy-induced snowpack began to melt, signs of life began to return to the neural net community. <a href="https://en.wikipedia.org/wiki/George_Boole">George Bool’s</a> great-great-grandson, a man by the name of <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">Geoff Hinton</a>,
 finished his PhD studying neural networks in 1978 and by 1986 had moved
 to the institute for cognitive science at UC San Diego. Along with <a href="https://en.wikipedia.org/wiki/David_Rumelhart">David Rumelhart</a> and <a href="http://www.ccs.neu.edu/home/rjw/">Ronald Williams</a>, Hinton published a paper entitled <a href="http://www.nature.com/nature/journal/v323/n6088/abs/323533a0.html">“Learning representations by back-propagating errors”</a>.
 In this paper they showed that neural nets with many hidden layers 
could be effectively trained by a relatively simple procedure. This 
would allow neural nets to get around the weakness of the perceptron 
because the additional layers endowed the network with the ability to 
learn nonlinear functions. Around the same time it was shown that such 
networks had the ability to learn <em>any</em> function, a result known as the <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"><strong>universal approximation theorem</strong></a>. And with that, neural nets were off to the races.</p>

<p>The algorithm works by taking the derivative of the network’s loss 
function and back-propagating the errors to update the parameters in the
 lower layers, hence the common moniker <strong>backprop</strong>. Though Rumelhart, Hinton and Williams are often credited for inventing backprop, this is <a href="http://people.idsia.ch/%7Ejuergen/who-invented-backpropagation.html">disputed</a> by some. There is something of a long standing controversy between the deep learning “conspiracy” of <a href="https://www.thestar.com/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence.html">Geoff Hinton</a>
/Yann Lecun/<a href="http://www.iro.umontreal.ca/%7Ebengioy/yoshua_en/">Yoshua Bengio</a> and <a href="http://people.idsia.ch/%7Ejuergen/">Jürgen Schmidhuber</a> as to who should actually be credited with several key developments, but that’s an issue best left tabled for now.</p>

<p>This algorithm lead to some early successes, notably in training <strong>convoluational neural nets</strong>
 (aka CNNs, also referred to using the less pronounceable convnets) to 
recognize handwritten digits, a project which was speared headed by Yann
 Lecun at AT&amp;T <a href="https://en.wikipedia.org/wiki/Bell_Labs">Bell Labs</a>. Below is a gif of Yann’s convnet reading a check:</p>

<p><img src="lenet_att.html" alt="Lenet" class="center-image" width="300px" height="200px"></p>

<p>Unfortunately, neural nets were in for another deep freeze. The approach didn’t scale to larger problems, and by the 90s the <a href="https://en.wikipedia.org/wiki/Support_vector_machine">support vector machine</a>
 (SVM) became the method of choice, and neural nets were moved back into
 storage. In hindsight, neural nets were simply an idea before their 
time, but it would be another 10-15 years before data and computing 
power were such that neural nets would finally be able to reach their <a href="http://knowyourmeme.com/memes/this-isnt-even-my-final-form">final form</a>.</p>

<h3 id="rebranding-as-deep-learning-2006">Rebranding as ‘Deep Learning’ (2006)</h3>
<p>Around 2006, Hinton once again declared that he knew <a href="https://www.youtube.com/watch?v=mlXzufEk-2E">how the brain works</a>, and introduced the idea of <a href="https://metacademy.org/graphs/concepts/unsupervised_pre_training">unsupervised pretraining</a> and <a href="https://en.wikipedia.org/wiki/Deep_belief_network">deep belief nets</a>. The idea was to train a simple 2-layer unsupervised model like a <a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine">restricted boltzman machine</a>, freeze all the parameters, stick on a new layer on top and train <em>just the parameters</em> for the new layer. You would keep adding and training layers in this <a href="https://en.wikipedia.org/wiki/Greedy_algorithm">greedy</a>
 fashion until you had a deep network, and then use the result of this 
process to initialize the parameters of a traditional neural network. 
Using this strategy, people were able to train networks that were deeper
 than previous attempts, prompting a rebranding of <strong>‘neural networks’ to ‘deep learning’</strong>. If you go to <a href="http://deeplearning.net/">deeplearning.net</a>, which I believe is owned by <a href="https://mila.umontreal.ca/en/">MILA</a>, the title proudly declares</p>

<blockquote>
  <p>Deep Learning… moving beyond shallow machine learning since 2006!</p>
</blockquote>

<p>in recognition of this transition.</p>

<h3 id="the-breakthrough-2012">The Breakthrough (2012)</h3>
<p>With the renewed interest brought on by unsupervised pre-training, 
more and more neural network papers began to trickle out of several 
research labs. Then came a relative break through using deep nets for <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/HintonDengYuEtAl-SPM2012.pdf">speech recognition</a> where, for one of the first times, a neural model achieved state of the art. There had been a saying, usually <a href="https://books.google.com/books?id=3zHtsuuIKTsC&amp;pg=PA98&amp;lpg=PA98&amp;dq=Neural+nets+are+the+second+best+way+to+do+almost+anything&amp;source=bl&amp;ots=SMFnQV_c-B&amp;sig=A2ZY2RUyODk_q2flapOf13aBOE8&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjh86OZxaTSAhXF34MKHY1QAS0Q6AEIHDAA#v=onepage&amp;q=Neural%20nets%20are%20the%20second%20best%20way%20to%20do%20almost%20anything&amp;f=false">attributed</a> to JS Denker, that went:</p>

<blockquote>
  <p>Neural networks are the second best way to do almost anything</p>
</blockquote>

<p>However, as neural nets started surpassing traditional methods, this 
became less and less true. Things would finally come to a head in 2012 
on the <a href="http://www.image-net.org/challenges/LSVRC/">Large Scale Visual Recognition Challenge</a>(LSVRC). In 2010, a large database known as <a href="http://imagenet.stanford.edu/papers/imagenet_cvpr09.pdf">Imagenet</a> containing millions of labeled images was created and published by <a href="http://vision.stanford.edu/feifeili/">Fei-Fei Li’s</a>
 group at Stanford. This database was coupled with the annual LSVRC, 
where contestants would build computer vision models, submit their 
predictions, and receive a score based on how accurate they were. In the
 first two years of the contest, the top models had error rates of 28% 
and 26%. In 2012, <a href="http://www.cs.toronto.edu/%7Ekriz/">Alex Krizhevsky</a>, <a href="http://www.cs.toronto.edu/%7Eilya/">Ilya Sutskever</a>,
 and Geoff Hinton entered a submission that would halve the existing 
error rate to 16%. The model combined several critical components that 
would go one to become mainstays in deep learning models.</p>

<p>Probably the most important piece was the use of graphics processing 
units (GPUs) to train the model. GPUs are essentially parallel 
floating-point calculators with 100s-1000s of cores. The speedup offered
 by GPUs meant they could train larger models, which led to lower error 
rates. They also introduced a method to reduce overfitting known as <a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf">dropout</a> and used the rectified linear activation unit (<a href="http://ieeexplore.ieee.org/abstract/document/6639346/">ReLU</a>),
 both of which are now bread and butter components in modern deep 
learning. The network went on to become known as “Alexnet” and the <a href="https://scholar.google.com/scholar?q=imagenet&amp;btnG=&amp;hl=en&amp;as_sdt=0%2C22">paper</a>
 describing it has been cited nearly 10,000 times since it was published
 at NIPS in 2012. Here is a graph showing the error rate on Imagenet 
over time, note the precipitous drop in 2012 and the huge uptick in 
teams using GPUs:</p>

<p><img src="imagenet_progress.html" alt="imagenet" class="center-image" width="763px" height="466px"></p>

<p>Many important innovations would come after this result, but I think 
it’s safe to say this is the moment when the deep learning levy finally 
broke. Many are becoming convinced that we are in the midst of a <a href="https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html?_r=0">Great AI Awakening</a>
 and billions are being spent by companies to stockpile AI talent and 
technology. It’s hard to know what will come next and the extent to 
which deep learning will play a role, but for now, it does feel like 
we’ve experience a paradigm shift in machine learning that is here to 
stay.</p>

<h3 id="whats-changed-why-now">What’s changed? Why now?</h3>
<p>Many of the core concepts for deep learning were in place by the 80s 
or 90s, so what happened in the past 5-7 years that changed things? 
Though there are many factors, the two most crucial components appear to
 be availability of <strong>massive labeled data sets</strong> and <strong>GPU computing</strong>. Here’s a run down of factors that seem to have had a role in the deep learning revolution:</p>

<ul>
  <li><strong>Appearance of large, high-quality labeled datasets</strong>
 - Data along with GPUs probably explains most of the improvements we’ve
 seen. Deep learning is a furnace that needs a lot of fuel to keep 
burning, and we finally have enough fuel.</li>
  <li><strong>Massively parallel computing with GPUs</strong> - It turns
 out that neural nets are actually just a bunch of floating point 
calculations that you can do in parallel. It also turns out that GPUs 
are <strong>great</strong> at doing these types of calculations. The 
transition from CPU-based training to GPU-based has resulted in massive 
speed ups for these models, and as a result, allowed us to go bigger and
 deeper, and with more data.</li>
  <li><strong>Backprop-friendly activation functions</strong> - The transition away from saturating activation functions like tanh and the logistic function to things like <a href="https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29">ReLU</a> have alleviated the <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">vanishing gradient problem</a></li>
  <li><strong>Improved architectures</strong> - <a href="https://arxiv.org/abs/1512.03385">Resnets</a>, <a href="https://www.cs.unc.edu/%7Ewliu/papers/GoogLeNet.pdf">inception modules</a>, and <a href="https://arxiv.org/abs/1505.00387">Highway networks</a> keep the gradients flowing smoothly, and let us increase the depth and flexibility of the network</li>
  <li><strong>Software platforms</strong> - Frameworks like <a href="https://www.tensorflow.org/">tensorflow</a>, <a href="http://deeplearning.net/software/theano/">theano</a>, <a href="http://chainer.org/">chainer</a>, and <a href="http://mxnet.io/">mxnet</a> that provide <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a>
 allow for seamless GPU computing and make protoyping faster and less 
error-prone. They let you focus on your model structure without having 
to worry about low-level details like gradients and GPU management.</li>
  <li><strong>New regularization techniques</strong> - Techniques like <a href="https://www.cs.toronto.edu/%7Ehinton/absps/JMLRdropout.pdf">dropout</a>, <a href="http://jmlr.org/proceedings/papers/v37/ioffe15.pdf">batch normalization</a>, and data-augmentation allow us to train larger and larger networks without (or with less) overfitting</li>
  <li><strong>Robust optimizers</strong> - <a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms">Modifications</a> of the SGD procedure including momentum, RMSprop, and <a href="https://arxiv.org/pdf/1412.6980.pdf">ADAM</a> have helped eek out every last percentage of your loss function.</li>
</ul>

<p>Most of these improvements have been driven by empirical performance 
on a standard set of benchmarks. There isn’t a ton of theoretical 
justification (though there is some) for many of these techniques, which
 leads to the following hypothesis:</p>

<blockquote>
  <p><strong>Deep Learning Hypothesis:</strong> The success of deep learning is largely a success of engineering.</p>
</blockquote>

<p>I don’t <em>think</em> this is a controversial position, and it’s not
 meant to minimize the success of deep learning, but I think it’s a fair
 characterization of how the state of the art has been pushed forward.</p>

<h3 id="how-can-you-stay-current">How can you stay current?</h3>
<p>This field moves fast, and papers get old on a timescale of <em>months</em>,
 so keeping your finger on the pulse can be tough. I’ve developed a few 
coping mechanisms to try and help drink from the deep learning firehose:</p>

<ul>
  <li>Read <a href="https://arxiv.org/">arXiv</a>, and specifically the <a href="https://arxiv.org/list/stat.ML/recent">machine learning</a>
 subsection. Close to 100% of deep learning papers get posted in some 
form on arXiv, so it will always be the definitive source for the 
cutting edge. The problem is that the volume of new work coming out is 
too large for any one person to consume. The main way to cope is a 
combination of social media like twitter and reddit and blogs from 
individuals and various research groups.</li>
  <li><a href="http://www.arxiv-sanity.com/">arxiv-sanity</a> is a machine learning (though not deep learning!) powered tool from <a href="http://cs.stanford.edu/people/karpathy/">Andrej Karpathy</a>
 to help you shift through the vast swaths of research that is produced 
daily. Makes it easy to find papers relevant to your interests and even 
comes with a Netflix style recommendation engine.</li>
  <li>The <a href="https://blog.keras.io/">keras blog</a> has a lot of great tutorials on how to implement state of the art models in keras.</li>
  <li>The <a href="https://www.reddit.com/r/MachineLearning/">machine learning subreddit</a>
 contains links to and discussions of the latest and greatest papers. 
Like all online communities, it can occasionally devolve into 
unproductive infighting, but I still find it to be tremendously useful.</li>
  <li>The <a href="https://openai.com/blog/">OpenAI</a> blog - Mainly focused on GANs and reinforcement learning</li>
  <li><a href="https://research.googleblog.com/">Google research</a> blog - All of Google research, but with a huge serving of deep learning since this is one of their main areas of focus.</li>
  <li>Subscribe to the <a href="http://www.thetalkingmachines.com/">talking machines</a> podcast. Co-hosted by <a href="http://people.seas.harvard.edu/%7Erpa/">Ryan Adam</a> and <a href="http://www.katherinelgorman.com/">Katherine Gorman</a>,
 talking machines is one of my favorite podcasts. Now two full seasons 
(with more to come!) of interviews with luminaries of the field and 
discussion of the latest and greatest papers. Highly recommended!</li>
  <li>Adrian Colyer’s <a href="https://blog.acolyer.org/category/machine-learning/">blog</a>
 ‘The Morning Paper’. Adrian write summaries of new CS papers pretty 
frequently, and many (if not most) are about recent events in deep 
learning. Definitely a great way to quickly get the Cliff’s notes for 
new papers.</li>
  <li><strong>Twitter</strong> - this is probably the <strong>best</strong>
 way to stay up to date, because everything happens in real time. Here 
is an incomplete list of who to follow to help get you started:
    <ul>
      <li><a href="https://twitter.com/ylecun">Yann Lecun</a> - Head of 
Facebook’s AI research lab. Giant of the field and tweets often. If 
you’re looking for some laughs or just want to #feelthelearn, check out 
the parody account <a href="https://twitter.com/boredyannlecun">Bored Yann Lecunn</a> run by <a href="https://twitter.com/jackclarkSF">Jack Clark</a>: <iframe id="twitter-widget-0" scrolling="no" allowtransparency="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 109px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2d991e3dfc9abb2549972ce8b64c5d85.en.html#dnt=false&amp;id=twitter-widget-0&amp;lang=en&amp;screen_name=ylecun&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1572315984942" data-screen-name="ylecun" frameborder="0"></iframe><script async="" src="widgets.js" charset="utf-8"></script></li>
      <li><a href="https://twitter.com/karpathy">Andrej Karpathy</a> - Research scientist at OpenAI, also has a great <a href="http://karpathy.github.io/">blog</a>: <iframe id="twitter-widget-1" scrolling="no" allowtransparency="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 119px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2d991e3dfc9abb2549972ce8b64c5d85.en.html#dnt=false&amp;id=twitter-widget-1&amp;lang=en&amp;screen_name=karpathy&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1572315984977" data-screen-name="karpathy" frameborder="0"></iframe><script async="" src="widgets.js" charset="utf-8"></script></li>
      <li><a href="https://twitter.com/goodfellow_ian">Ian Goodfellow</a> - Researcher at OpenAI, creator of generative adversarial networks(i.e. <a href="https://arxiv.org/abs/1406.2661">GANs</a>), explainer of <a href="https://arxiv.org/abs/1412.6572">adversarial examples</a>, world’s most trustworthy last name: <iframe id="twitter-widget-2" scrolling="no" allowtransparency="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 151px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2d991e3dfc9abb2549972ce8b64c5d85.en.html#dnt=false&amp;id=twitter-widget-2&amp;lang=en&amp;screen_name=goodfellow_ian&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1572315984982" data-screen-name="goodfellow_ian" frameborder="0"></iframe><script async="" src="widgets.js" charset="utf-8"></script></li>
      <li><a href="https://twitter.com/fchollet">Francois Chollet</a> - Creator of keras, researcher at Google: <iframe id="twitter-widget-3" scrolling="no" allowtransparency="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 112px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2d991e3dfc9abb2549972ce8b64c5d85.en.html#dnt=false&amp;id=twitter-widget-3&amp;lang=en&amp;screen_name=fchollet&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1572315984990" data-screen-name="fchollet" frameborder="0"></iframe><script async="" src="widgets.js" charset="utf-8"></script></li>
      <li><a href="https://twitter.com/rsalakhu">Russ Salakhutdinov</a> - Director of AI Research at Apple and professor at CMU: <iframe id="twitter-widget-4" scrolling="no" allowtransparency="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 118px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2d991e3dfc9abb2549972ce8b64c5d85.en.html#dnt=false&amp;id=twitter-widget-4&amp;lang=en&amp;screen_name=rsalakhu&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1572315985005" data-screen-name="rsalakhu" frameborder="0"></iframe><script async="" src="widgets.js" charset="utf-8"></script></li>
      <li><a href="https://twitter.com/drfeifei">Fei-Fei Li</a> - Imagenet creator, Stanford professor and director of the Stanford AI lab, Chief Scientist at Google Cloud: <iframe id="twitter-widget-5" scrolling="no" allowtransparency="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 111px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2d991e3dfc9abb2549972ce8b64c5d85.en.html#dnt=false&amp;id=twitter-widget-5&amp;lang=en&amp;screen_name=drfeifei&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1572315985011" data-screen-name="drfeifei" frameborder="0"></iframe><script async="" src="widgets.js" charset="utf-8"></script></li>
      <li><a href="https://twitter.com/AndrewYNg">Andrew Ng</a> - Cofounder of coursera, former Standford professor, Chief scientist at Baidu: <iframe id="twitter-widget-6" scrolling="no" allowtransparency="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 138px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2d991e3dfc9abb2549972ce8b64c5d85.en.html#dnt=false&amp;id=twitter-widget-6&amp;lang=en&amp;screen_name=AndrewYNg&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1572315985021" data-screen-name="AndrewYNg" frameborder="0"></iframe><script async="" src="widgets.js" charset="utf-8"></script></li>
      <li><a href="https://twitter.com/sedielem">Sander Dieleman</a> - Creator of the <a href="https://lasagne.readthedocs.io/en/latest/">lasagne</a> deep learning framework and research scientist at Deep Mind. Also has a great <a href="http://benanne.github.io/">blog</a> with some epic Kaggle <a href="http://benanne.github.io/2015/03/17/plankton.html">contest</a> <a href="http://benanne.github.io/2014/04/05/galaxy-zoo.html">write ups</a>. You can learn a lot just by reading how Sander approaches a Kaggle contest: <iframe id="twitter-widget-7" scrolling="no" allowtransparency="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 119px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2d991e3dfc9abb2549972ce8b64c5d85.en.html#dnt=false&amp;id=twitter-widget-7&amp;lang=en&amp;screen_name=sedielem&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1572315985030" data-screen-name="sedielem" frameborder="0"></iframe><script async="" src="widgets.js" charset="utf-8"></script></li>
      <li><a href="https://twitter.com/hugo_larochelle">Hugo Larochelle</a> - Google Brain researcher: <iframe id="twitter-widget-8" scrolling="no" allowtransparency="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 153px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2d991e3dfc9abb2549972ce8b64c5d85.en.html#dnt=false&amp;id=twitter-widget-8&amp;lang=en&amp;screen_name=hugo_larochelle&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1572315985045" data-screen-name="hugo_larochelle" frameborder="0"></iframe><script async="" src="widgets.js" charset="utf-8"></script></li>
      <li><a href="https://twitter.com/math_rachel">Rachel Thomas</a> - Cofounder of <a href="http://course.fast.ai/">fast.ai</a>, a fantastic deep learning MOOC: <iframe id="twitter-widget-9" scrolling="no" allowtransparency="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 136px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2d991e3dfc9abb2549972ce8b64c5d85.en.html#dnt=false&amp;id=twitter-widget-9&amp;lang=en&amp;screen_name=math_rachel&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1572315985053" data-screen-name="math_rachel" frameborder="0"></iframe><script async="" src="widgets.js" charset="utf-8"></script></li>
      <li><a href="https://twitter.com/AndrewLBeam">Andrew Beam</a> - 
Your humble narrator. I’m not as prolific a tweeter as some on this 
list, but I try to share papers and projects I find interesting and/or 
important, with a special focus on healthcare and medicine: <iframe id="twitter-widget-10" scrolling="no" allowtransparency="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 150px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2d991e3dfc9abb2549972ce8b64c5d85.en.html#dnt=false&amp;id=twitter-widget-10&amp;lang=en&amp;screen_name=AndrewLBeam&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1572315985069" data-screen-name="AndrewLBeam" frameborder="0"></iframe><script async="" src="widgets.js" charset="utf-8"></script></li>
    </ul>
  </li>
</ul>

<h3 id="additional-references">Additional References:</h3>

<ul>
  <li><a href="https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training/"><strong>Deep Learning in a Nutshell: History and Training</strong></a> from NVIDIA</li>
  <li><a href="https://medium.com/@Jaconda/a-concise-history-of-neural-networks-2070655d3fec#.alhfvwwl2"><strong>A Concise History of Neural Networks</strong></a> - A well-written summary from Jaspreet Sandhu of the major milestones in the development of neural networks</li>
  <li><a href="http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/"><strong>A ‘Brief’ History of Neural Nets and Deep Learning</strong></a> - An epic, multipart series from Andrey Kurenkov on the history of deep learning that I highly recommend.</li>
</ul>

<div id="disqus_thread"><iframe id="dsq-app6829" name="dsq-app6829" allowtransparency="true" scrolling="no" tabindex="0" title="Disqus" style="width: 1px !important; min-width: 100% !important; border: medium none !important; overflow: hidden !important; height: 1589px !important;" src="https://disqus.com/embed/comments/?base=default&amp;f=beamandrewgithubio&amp;t_u=https%3A%2F%2Fbeamandrew.github.io%2Fdeeplearning%2F2017%2F02%2F23%2Fdeep_learning_101_part1.html&amp;t_d=Deep%20Learning%20101%20-%20Part%201%3A%20History%20and%20Background&amp;t_t=Deep%20Learning%20101%20-%20Part%201%3A%20History%20and%20Background&amp;s_o=default#version=049b9d4c8356e0d7fe6487ff57f30ea3" horizontalscrolling="no" verticalscrolling="no" frameborder="0" width="100%"></iframe></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'beamandrewgithubio';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

  </div>
</div>
<div class="two spacing"></div>
<div class="four spacing"></div>


    </div>
    <div class="large-3 columns" id="sidebar">
      
<div class="links">
  <h3>Categories</h3>
  <ul>
    
      <li><a href="#">Deeplearning</a></li>
    
      <li><a href="#">Convolutional neural nets</a></li>
    
      <li><a href="#">Medical imaging</a></li>
    
      <li><a href="#">Healthcare</a></li>
    
      <li><a href="#">Preterm birth</a></li>
    
      <li><a href="#">Obstetrics</a></li>
    
  </ul>
</div>
<div class="links">
  <h3>Recent Posts</h3>
  <ul>
    
      
      <li><a href="https://beamandrew.github.io/healthcare/preterm%20birth/obstetrics/2017/10/20/17P_post.html">Pricing and the orphan drug act: the curious case of 17p</a></li>
    
      
      <li><a href="https://beamandrew.github.io/deeplearning/2017/06/04/deep_learning_works.html">You can probably use deep learning even if your data isn't that big</a></li>
    
      
      <li><a href="https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part2.html">Deep learning 101 - part 2: multilayer perceptrons</a></li>
    
      
      <li><a href="https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html">Deep learning 101 - part 1: history and background</a></li>
    
      
      <li><a href="https://beamandrew.github.io/deeplearning/2016/12/12/nips-2016.html">Nips 2016</a></li>
    
      
      <li><a href="https://beamandrew.github.io/deeplearning/convolutional%20neural%20nets/medical%20imaging/2016/08/20/kaggle-segmentation.html">Segmenting the brachial plexus with deep learning</a></li>
    
  </ul>
</div>
<div class="links">
  <h3>Tags</h3>
  <ul>
    
  </ul>
</div>
<div class="four spacing"></div>

    </div>
  </div>
</div>

    </div>

    <div id="footer">
  <div class="three spacing"></div>
  <div class="row">
    <div class="large-3 medium-3 columns">
      <h1>
        <a href="https://beamandrew.github.io/deeplearning/2017/02/23/index.html"><img alt="" src="logo-grey.html"></a>
        <a href="https://www.hsph.harvard.edu/"><img alt="" src="hsph-logo.html"></a>
      </h1>
      <p>©2019 beamlab. All rights reserved.
      <br>
      Harvard School of Public Health
      <br>
      820C Kresge Hall
      <br>
      677 Huntington Ave.
      <br>
      Boston, MA 02115
      </p>
      <div class="two spacing"></div>
    </div>
    <div class="large-3 medium-3 columns">
      <div class="spacing"></div>
      <div class="links">
        <h4>Site map</h4>
        <ul>
          <li><a href="https://beamandrew.github.io/deeplearning/2017/02/23/index.html">Home</a></li>
          <li><a href="https://beamandrew.github.io/deeplearning/2017/02/23/overview.html">Vision</a></li>
          <li><a href="https://beamandrew.github.io/deeplearning/2017/02/23/team.html">Team</a></li>
          <li><a href="https://beamandrew.github.io/deeplearning/2017/02/23/positions.html">Positions</a></li>
          <li><a href="https://beamandrew.github.io/deeplearning/2017/02/23/blog">Follow our blog</a></li>
        </ul>
      </div>
      <div class="spacing"></div>
    </div>
    <div class="large-3 medium-3 columns">
      <div class="spacing"></div>
      <div class="links">
        <h4>Recent posts</h4>
        <ul>
          
            <li><a href="https://beamandrew.github.io/healthcare/preterm%20birth/obstetrics/2017/10/20/17P_post.html">Pricing and the Orphan Drug Act: The Curious Case of 17P</a></li>
          
            <li><a href="https://beamandrew.github.io/deeplearning/2017/06/04/deep_learning_works.html">You can probably use deep learning even if your data isn't that big</a></li>
          
            <li><a href="https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part2.html">Deep Learning 101 - Part 2: Multilayer Perceptrons</a></li>
          
            <li><a href="https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html">Deep Learning 101 - Part 1: History and Background</a></li>
          
            <li><a href="https://beamandrew.github.io/deeplearning/2016/12/12/nips-2016.html">NIPS 2016</a></li>
          
            <li><a href="https://beamandrew.github.io/deeplearning/convolutional%20neural%20nets/medical%20imaging/2016/08/20/kaggle-segmentation.html">Segmenting the Brachial Plexus with Deep Learning</a></li>
          
        </ul>
      </div>
      <div class="spacing"></div>
    </div>
    <div class="large-3 medium-3 columns">
      <div class="spacing"></div>
      <h4>Find us</h4>
      <div class="spacing"></div>
      <ul class="socials">
        <li>
          <a href="http://github.com/beamandrew">
            <i class="fa fa-github"></i>
          </a>
        </li>
        <li>
          <a href="http://twitter.com/AndrewLBeam">
            <i class="fa fa-twitter"></i>
          </a>
        </li>
        <li>
          <a href="https://scholar.google.com/citations?user=SgHOsrsAAAAJ&amp;hl=en">
            <i class="fa fa-google"></i>
          </a>
        </li>
      </ul>
      <div class="spacing"></div>
    </div>
  </div>
  <div class="two spacing"></div>
</div>

<script src="jquery_005.js" type="text/javascript"></script>
<script src="jquery_002.js" type="text/javascript"></script>
<script src="jquery_003.js" type="text/javascript"></script>
<script src="jquery.js" type="text/javascript"></script>
<script src="jquery_004.js" type="text/javascript"></script>
<script src="app.js" type="text/javascript"></script>

  


<iframe style="display: none;"></iframe><iframe scrolling="no" allowtransparency="true" src="https://platform.twitter.com/widgets/widget_iframe.2d991e3dfc9abb2549972ce8b64c5d85.html?origin=https%3A%2F%2Fbeamandrew.github.io" title="Twitter settings iframe" style="display: none;" frameborder="0"></iframe><iframe id="rufous-sandbox" scrolling="no" allowtransparency="true" allowfullscreen="true" style="position: absolute; visibility: hidden; display: none; width: 0px; height: 0px; padding: 0px; border: medium none;" title="Twitter analytics iframe" frameborder="0"></iframe></body></html>